,doc,date,co
0,"Systems, methods, and apparatus for accessing distributed models in automated machine processing, including using large language models in machine translation, speech recognition and other applications.",20140523,Google
1,"A data processing method includes obtaining text information corresponding to a presented content, the presented content comprising a plurality of areas; performing text analysis on the text information to obtain a first keyword sequence, the first keyword sequence including area keywords associated with at least one area of the plurality of areas; obtaining speech information related to the presented content, the speech information at least comprising a current speech segment; and using a first model network to perform analysis on the current speech segment to determine the area corresponding to the current speech segment, wherein the first model network comprises the first keyword sequence.",20130716,IBM
2,"Automatically creating word breakers which segment words into morphemes is described, for example, to improve information retrieval, machine translation or speech systems. In embodiments a cross-lingual phrase table, comprising source language (such as Turkish) phrases and potential translations in a target language (such as English) with associated probabilities, is available. In various examples, blocks of source language phrases from the phrase table are created which have similar target language translations. In various examples, inference using the target language translations in a block enables stem and affix combinations to be found for source language words without the need for input from human-judges or prior knowledge of source language linguistic rules or a source language lexicon.",20130411,Microsoft
3,"Methods and apparatus are provided for controlling a function based on speech recognition. Speech input in a first language is recognized. Dictation, which converts the speech input into text based on the first language, is performed. A language change event is detected. Additional speech input in a second language, which is different from the first language, is recognized after the language change event. Dictation, which converts the additional speech input into additional text based on the second language, is performed.",20140325,SAMSUNG
4,"According to one embodiment, a speech translation apparatus includes a speech recognition unit, a translation unit, a search unit and a selection unit. The speech recognition unit successively performs speech recognition to obtain a first language word string. The translation unit translates the first language word string into a second language word string. The search unit search for at least one similar example and acquires the similar example and a translation example. The selection unit selects, in accordance with a user instruction, at least one of the first language word string associated with the similar example and the second language word string associated with the translation example, as a selected word string.",20130409,TOSHIBA
5,"According to one embodiment, a content creation support apparatus includes a speech synthesis unit, a speech recognition unit, an extraction unit, a detection unit, a presentation unit and a selection unit. The speech synthesis unit performs a speech synthesis on a first text. The speech recognition unit performs a speech recognition on the synthesized speech to obtain a second text. The extraction unit extracts feature values by performing a morphological analysis on each of the first and second texts. The detection unit compares a first feature value of a first difference string and a second feature value of a second difference string. The presentation unit presents correction candidate(s) according to the second feature value. The selection unit selects one of the correction candidates in accordance with an instruction from a user.",20140611,TOSHIBA
6,"A first speech input device captures a speech of a first language. A first speech output device outputs another speech of the first language. A second speech input device captures a speech of a second language. A second speech output device outputs another speech of the second language. In a speech recognition/translation server, a first speech recognition device receives a first utterance speech of the first language from the first speech input device, and recognizes the first utterance speech. A first machine translation device consecutively translates the first language of the recognition result into the second language without waiting completion of the first utterance speech. A first speech synthesis device generates a second speech of the translation result. A first output adjustment device outputs the first utterance speech and the second speech to the second speech output device by adjusting a volume of the first utterance speech to be smaller than a volume of the second speech.",20140916,TOSHIBA
7,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20150623,Google
8,"Embodiments that relate to identifying potential cross-language speech recognition problems are disclosed. For example, in one disclosed embodiment a speech recognition problem detection program receives a target word in a non-native language from a target application. A phonetic transcription of the target word comprising a plurality of target phonetic units is acquired. The program determines that at least one of the target phonetic units is not found in a plurality of native phonetic units associated with a native language. In response, a warning of the potential cross-language speech recognition problem may be outputted for display on a display device. The warning may comprise the target word.",20131106,Microsoft
9,"Systems and methods are provided for translating a phrase block and presenting meanings and translations of the phrase block. Consistent with certain embodiments, computer-implemented systems and methods are provided for identifying parts of speech of the phrase block, determining definitions of the phrase block in the source language corresponding to the parts of speech, and determining usage examples corresponding to the definitions. Additionally, systems and methods are provided for translating the phrase block from the source language to translated text and for ranking the parts of speech based on usage. Consistent with certain embodiments, computer-implemented systems and methods are also provided for determining usage based on a user's location, based on a corpus of documents, and/or based on content stored by the user. Systems and methods are also provided for displaying the phrase block, definitions and usage examples of the phrase block, and/or translations of the phrase block according to the ranking.",20141208,Google
10,"According to one embodiment, a speech translation system includes a first terminal device including a first speech input unit for inputting a first speech of a first language spoken by a first user, and converting the first speech to a first speech signal; a second terminal device including a second speech input unit for inputting a second speech of a second language spoken by a second user, and converting the second speech to a second speech signal; a speech recognition device that receives the first speech signal and the second speech signal, recognizes the first speech signal to a first recognized text, and recognizes the second speech signal to a second recognized text; a machine translation device that receives the first recognized text and the second recognized text, translates the first recognized text to a first translated text of the second language, and translates the second recognized text to a second translated text of the first language; and a control device; wherein the first terminal device receives (a) a first text set of the first language being the first recognized text and the second translated text, and (b) a second text set of the second language being the second recognized text and the first translated text, and includes a first display unit that displays the first text set and the second text set; and the second terminal device receives at least one text of the second text set, and includes a second display unit that displays the at least one text of the second text set.",20140923,TOSHIBA
11,"The disclosed subject matter provides a system, computer readable storage medium, and a method providing an audio and textual transcript of a communication. A conferencing services may receive audio or audio visual signals from a plurality of different devices that receive voice communications from participants in a communication, such as a chat or teleconference. The audio signals representing voice (speech) communications input into respective different devices by the participants. A translation services server may receive over a separate communication channel the audio signals for translation into a second language. As managed by the translation services server, the audio signals may be converted into textual data. The textual data may be translated into text of different languages based the language preferences of the end user devices in the teleconference. The translated text may be further translated into audio signals.",20150817,Google
12,"A method and system for analyzing language morphology to facilitate statistical machine translation from a first language to a target language is disclosed. The method and system can include marking words in a first language text with a part-of-speech tag, and parsing one or more sentences in the first language text to determine syntactic dependence relations between the words in the one or more sentences of the first language text. Morphological features of the words in the first language text can also be extracted. The method and system can also include tagging the words in the first language text with a language morphology tag from a second language. A modified form of words in the first language text can be generated using the language morphology tags from the second language. The modified form of the words can be used to facilitate translation from the first language to a target language.",20131204,IBM
13,The customization of language modeling components for speech recognition is provided. A list of language modeling components may be made available by a computing device. A hint may then be sent to a recognition service provider for combining the multiple language modeling components from the list. The hint may be based on a number of different domains. A customized combination of the language modeling components based on the hint may then be received from the recognition service provider.,20140327,Microsoft
14,"According to an embodiment, a speech translation apparatus includes an allocator, a searcher and a sorter. The allocator allocates, to each of the phrases in the set of phrases, a weight dependent on a difference between a current dialog status and a dialog status associated with an original speech sound that corresponds to a text in which each of the phrases appears. The searcher searches the plurality of examples in the first language for an example including one or more phrases included in the set of phrases to obtain a hit example set. The sorter calculates a score of each of hit examples included in the hit example set based on the weight and the degree of similarity to sort the hit examples based on the score.",20141223,TOSHIBA
15,"In language evaluation systems, user expressions are often evaluated by speech recognizers and language parsers, and among several possible translations, a highest-probability translation is selected and added to a dialogue sequence. However, such systems may exhibit inadequacies by discarding alternative translations that may initially exhibit a lower probability, but that may have a higher probability when evaluated in the full context of the dialogue, including subsequent expressions. Presented herein are techniques for communicating with a user by formulating a dialogue hypothesis set identifying hypothesis probabilities for a set of dialogue hypotheses, using generative and/or discriminative models, and repeatedly re-ranks the dialogue hypotheses based on subsequent expressions. Additionally, knowledge sources may inform a model-based with a pre-knowledge fetch that facilitates pruning of the hypothesis search space at an early stage, thereby enhancing the accuracy of language parsing while also reducing the latency of the expression evaluation and economizing computing resources.",20131121,Microsoft
16,"According to one embodiment, a simultaneous speech processing apparatus includes an acquisition unit, a speech recognition unit, a detection unit and an output unit. The acquisition unit acquires a speech signal. The speech recognition unit generates a decided character string and at least one candidate character string. The detection unit detects a first character string as a processing piece character string if the first character string included in the decided character string exists commonly in one or more combined character strings on dividing the one or more combined character strings by a boundary indicating a morphological position serving as a start position of a processing piece in natural language processing. The output unit outputs the processing piece character string.",20140919,TOSHIBA
17,"Systems and methods are provided for improving language models for speech recognition by adapting knowledge sources utilized by the language models to session contexts. A knowledge source, such as a knowledge graph, is used to capture and model dynamic session context based on user interaction information from usage history, such as session logs, that is mapped to the knowledge source. From sequences of user interactions, higher level intent sequences may be determined and used to form models that anticipate similar intents but with different arguments including arguments that do not necessarily appear in the usage history. In this way, the session context models may be used to determine likely next interactions or “turns” from a user, given a previous turn or turns. Language models corresponding to the likely next turns are then interpolated and provided to improve recognition accuracy of the next turn received from the user.",20140618,Microsoft
18,"According to one embodiment, a speech of a first language is recognized using a speech recognition dictionary to recognize the first language and a second language, and a source sentence of the first language is generated. The source sentence is translated into a second language, and a translation sentence of the second language is generated. An unknown word included in the translation sentence is detected. The unknown word is not stored in the speech recognition dictionary. A first pronunciation candidate of the unknown word is estimated, from a representation of the unknown word. A second pronunciation candidate of the unknown word is estimated from a pronunciation of an original word included in the source sentence corresponding to the unknown word. The unknown word, the first pronunciation candidate and the second pronunciation candidate, are registered into the speech recognition dictionary correspondingly.",20140912,TOSHIBA
19,"According to one embodiment, a speech translation apparatus includes a speech recognition unit, a translation unit, a search unit and a selection unit. The speech recognition unit successively performs speech recognition to obtain a first language word string. The translation unit translates the first language word string into a second language word string. The search unit search for at least one similar example and acquires the similar example and a translation example. The selection unit selects, in accordance with a user instruction, at least one of the first language word string associated with the similar example and the second language word string associated with the translation example, as a selected word string.",20150326,TOSHIBA
20,"A method, system and computer program product for presenting tags of a tag cloud in a more understandable and visually appealing manner. Tags of a tag cloud that are associated with an object (e.g., web page) are retrieved. The retrieved tags are then assigned to parts of speech (e.g., noun, verb, adjective, adverb). Combinations of the tags are then generated based on the parts of speech assigned to the tags. For example, the combinations of the tags may be based on a template, such as <NOUN> <VERB> <ADJECTIVE>, <PRONOUN> <VERB> <ADJECTIVE>, <PRONOUN> is <ADVERB> <VERB> and so forth. The combinations of the tags are then presented after determining the layout to display the generated combinations of tags. Since the tags of the tag cloud are presented in a combination based on the parts of speech assigned to the tags, the tag cloud is more understandable and visually appealing.",20140108,IBM
21,"A server and method for correcting an error of a voice recognition result are provided. The method includes, in response to recognizing a user voice, determining a pattern of parts of speech of text data corresponding to the recognized user voice; comparing a prestored standard pattern of parts of speech with the pattern of parts of speech of text data; detecting an error region of the recognized user voice based on a result of the comparing; and correcting the text data corresponding to the detected error region.",20141224,SAMSUNG
22,"Architecture for playing a document converted into an audio format to a user of an audio-output capable device. The user can interact with the device to control play of the audio document such as pause, rewind, forward, etc. In more robust implementation, the audio-output capable device is a mobile device (e.g., cell phone) having a microphone for processing voice input. Voice commands can then be input to control play (“reading”) of the document audio file to pause, rewind, read paragraph, read next chapter, fast forward, etc. A communications server (e.g., email, attachments to email, etc.) transcodes text-based document content into an audio format by leveraging a text-to-speech (TTS) engine. The transcoded audio files are then transferred to mobile devices through viable transmission channels. Users can then play the audio-formatted document while freeing hand and eye usage for other tasks.",20150810,Microsoft
23,"Some implementations include a computer-implemented method. The method can include providing a training set of text samples to a semantic parser that associates text samples with actions. The method can include obtaining, for each of one or more of the text samples of the training set, data that indicates one or more domains that the semantic parser has associated with the text sample. For each of one or more domains, a subset of the text samples of the training set can be generated that the semantic parser has associated with the domain. Using the subset of text samples associated with the domain, a language model can be generated for one or more of the domain. Speech recognition can be performed on an utterance using the one or more language models that are generated for the one or more of the domains.",20140401,Google
24,"The disclosure pertains to a communication system for effecting a voice or video call between at least a source user speaking a source language and a target user speaking a target language. A translation procedure is performed on call audio of the call to generate an audio translation of the source user's speech in the target language for outputting to the target user. A notification is outputted to the target user to notify the target user of a change in the behaviour of the translation procedure, the change relating to the generation of the translation.",20150213,Microsoft
25,"According to one embodiment, a markup assistance apparatus includes an acquisition unit, a first calculation unit, a detection unit and a presentation unit. The acquisition unit acquires a feature amount for respective tags, each of the tags being used to control text-to-speech processing of a markup text. The first calculation unit calculates, for respective character strings, a variance of feature amounts of the tags which are assigned to the character string in a markup text. The detection unit detects a first character string assigned a first tag having the variance not less than a first threshold value as a first candidate including the tag to be corrected. The presentation unit presents the first candidate.",20150115,TOSHIBA
26,"Call audio of a call between a source user speaking a source language and a target user speaking a target language is received from a remote source user device of a source user via a communication network of a communication system, the call audio comprising speech of the source user in the source language. An automatic speech recognition procedure is performed on the call audio. A translation of the source user's speech is generated in the target language using the results of the speech recognition procedure. A translated synthetic speech audio version of the source user's speech is mixed with the source user's call audio and/or with translated audio of the target user's speech in the source language. The mixed audio signal is transmitted to a remote target user device of the target user via the communication network for outputting to at least the target user during the call.",20150211,Microsoft
27,"A computer-implemented technique can include receiving a media feed from a speaker computing device representing speech of a speaker user captured by the speaker computing device. The technique can include receiving a plurality of translation requests, each translation request being received from a listener computing device associated with a listener user and corresponding to a request to obtain a translated version of the media feed into a preferred language of the listener user. The technique can include determining the preferred language for each listener user. The technique can include obtaining a machine translated media feed for each of the translation requests, the machine translated media feed corresponding to a translation of the media feed from the source language to the preferred language of the listener user associated with the translation request. The technique can also include outputting the machine translated media feeds to the listener computing devices.",20140529,Google
28,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for recognizing speech in an utterance. The methods, systems, and apparatus may include actions of obtaining a candidate transcription including a sequence of words and generating morphological variants of one or more of the words from the candidate transcription. Additional actions may include, for each morphological variant, generating one or more additional candidate transcriptions that each include the morphological variant. Further actions may include generating respective language model scores for the candidate transcription and the one or more additional candidate transcriptions. Additional actions may include selecting a particular transcription from among the candidate transcription and the one or more additional candidate transcriptions, based on the language model scores.",20140226,Google
29,"Technologies described herein relate to modifying visual content for presentment on a display to facilitate improving performance of an automatic speech recognition (ASR) system. The visual content is modified to move elements further away from one another, wherein the moved elements give rise to ambiguity from the perspective of the ASR system. The visual content is modified to take into consideration accuracy of gaze tracking. When a user views an element in the modified visual content, the ASR system is customized as a function of the element being viewed by the user.",20140606,Microsoft
30,An input signal that includes linguistic content in a first language may be received by a computing device. The linguistic content may include text or speech. The computing device may associate the linguistic content in the first language with one or more phonemes from a second language. The computing device may also determine a phonemic representation of the linguistic content in the first language based on use of the one or more phonemes from the second language. The phonemic representation may be indicative of a pronunciation of the linguistic content in the first language according to speech sounds of the second language.,20140521,Google
31,"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for multilingual prosody generation. In some implementations, data indicating a set of linguistic features corresponding to a text is obtained. Data indicating the linguistic features and data indicating the language of the text are provided as input to a neural network that has been trained to provide output indicating prosody information for multiple languages. The neural network can be a neural network having been trained using speech in multiple languages. Output indicating prosody information for the linguistic features is received from the neural network. Audio data representing the text is generated using the output of the neural network.",20131230,Google
32,A language translation application on a user device includes a user interface that provides relevant textual and graphical feedback mechanisms associated with various states of voice input and translated speech.,20131108,Google
33,"Described is a technique for establishing an interaction language for a user interface without having to communicate with the user in a default language, which the user may or may not understand. The technique may prompt the user for multiples responses in order to determine a specific language. The responses may include speech input or selecting particular regions on a map. In some implementations, the language may be precise to a particular dialect or variant preferred or spoken by the user. Accordingly, this approach provides an accurate and efficient method of providing a high degree of specificity for language selection without overwhelming the user with an unmanageable list of languages.",20131231,Google
34,"A data processing apparatus receives data indicating a movement of a client device by a first user. The apparatus determines that the movement of the client device is a delimiter motion for switching between a first mode, in which the client device is configured to (i) provide a first interface for a first user speaking in a first language and (ii) perform speech recognition of the first language, and a second mode, in which the client device is configured to (i) provide a second interface for a second user speaking in a second language and (ii) perform speech recognition of the second language, the second interface being different from the first interface. Based on determining that the movement is a delimiter motion, the apparatus switches between the first mode and the second mode without the second user physically interacting with the client device.",20131112,Google
35,"A first speech processing device includes a first speech input unit and a first speech output unit. A second speech processing device includes a second speech input unit and a second speech output unit. In a server therebetween, a speech of a first language sent from the first speech input unit is recognized. The speech recognition result is translated into a second language. The translation result is back translated into the first language. A first speech synthesis signal of the back translation result is sent to the first speech output unit. A second speech synthesis signal of the translation result is sent to the second speech output unit. Duration of the second speech synthesis signal or the first speech synthesis signal is measured. The first speech synthesis signal and the second speech synthesis signal are outputted by synchronizing a start time and an end time thereof, based on the duration.",20140912,TOSHIBA
36,"Gestural annotation is described, for example where sensors such as touch screens and/or cameras monitor document annotation events made by a user of a document reading and/or writing application. In various examples the document annotation events comprise gestures recognized from the sensor data by a gesture recognition component. For example, the gestures may be in-air gestures or touch screen gestures. In examples, a compressed record of the sensor data is computed using at least the recognized gestures, document state and timestamps. In some examples the compressed record of the sensor data is used to facilitate consumption of the annotation events in relation to the document by a second user. In some examples the sensor data comprises touch sensor data representing electronic ink; and in some examples the sensor data comprises audio data capturing speech of a user.",20140228,Microsoft
37,"According to one embodiment, a speech translation apparatus includes a recognizer, a detector, a convertor and a translator. The recognizer recognizes a speech in a first language to generate a recognition result. The detector detects translation segments suitable for machine translation from the recognition result to generate translation-segmented character strings that are obtained by dividing the recognition result based on the detected translation segments. The convertor converts the translation-segmented character strings into converted character strings which are expressions suitable for the machine translation. The translator translates the converted character strings into a second language which is different from the first language to generate translated character strings.",20150908,TOSHIBA
38,"According to one embodiment, a speech translation apparatus includes a speech recognizer, a detector, a machine translator and a controller. The speech recognizer performs a speech recognition processing in chronological order on utterances of at least one first language made by a plurality of speakers to obtain a recognition text as a speech recognition result. The detector detects segments of meaning of the recognition text to obtain segments of text. The machine translator translates the segments of text into a second language different from the first language to obtain translated texts. The controller controls, if an utterance overlaps with another utterance in the chronological order, an order of displaying the translated texts corresponding to the overlapped utterances.",20150909,TOSHIBA
39,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20160217,Google
40,"There is provided an apparatus comprising at least one processor and a memory comprising code that, when executed on the at least one processor, causes the apparatus to receive an input user setting relating to relative volumes of the speech data in a preferred language and speech data in a non-preferred language when the speech data is played-out; and cause play-out of received speech data so that the volume of the played-out speech data is set in dependence on the user input and whether the received speech data is in the preferred language or the non-preferred language.",20141212,Microsoft
41,Embodiments relate to facilitating a meeting. A method for facilitating a meeting of a group of participants is provided. The method generates a graph of words from speeches of the participants as the words are received from the participants. The method partitions the group of participants into a plurality of subgroups of participants. The method performs a graphical text analysis on the graph to identify a cognitive state for each participant and a cognitive state for each subgroup of participants. The method informs at least one of the participants about the identified cognitive state of a participant or a subgroup of participants.,20150618,IBM
42,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20160105,Google
43,"A method and apparatus for speech recognition are disclosed. The speech recognition apparatus includes a processor configured to process a received speech signal, generate a word sequence based on a phoneme sequence generated from the speech signal, generate a syllable sequence corresponding to a word element among words comprised in the word sequence based on the phoneme sequence, and determine a text corresponding to a recognition result of the speech signal based on the word sequence and the syllable sequence.",20150626,SAMSUNG
44,"Natural language processing is provided. A unigram of a portion of text is determined, wherein the portion of text comprises a plurality of words. An initial confidence level of the unigram is determined, wherein the initial confidence level represents a probability that the unigram is of a semantic type identified by the initial confidence level. An expanded n-gram of the portion of text is determined, based, at least in part, on the unigram. Semantic analysis is performed on the expanded n-gram. At least one part of speech of the expanded n-gram is identified. Based, at least in part, on the initial confidence level, the semantic analysis, and the at least one part of speech, an adjusted confidence level of the expanded n-gram is determined.",20140710,IBM
45,"Implementations of the present disclosure include actions of providing first text for display on a computing device of a user, the first text being provided from a first speech recognition engine based on first speech received from the computing device, and being displayed as a search query, receiving a speech correction indication from the computing device, the speech correction indication indicating a portion of the first text that is to be corrected, receiving second speech from the computing device, receiving second text from a second speech recognition engine based on the second speech, the second speech recognition engine being different from the first speech recognition engine, replacing the portion of the first text with the second text to provide a combined text, and providing the combined text for display on the computing device as a revised search query.",20160428,Google
46,"A method, system and computer program product for assisting users to generate the desired meme in a document. A document is scanned to identify parts of speech, numerical text or numbers and images (collectively the “terms”) that provide positive/negative memes. A score for each of these terms is assigned. A score may then be assigned to each designated primary object (e.g., individual, company) discussed in the document using the scores assigned to the identified terms. Options may then be provided to modify the document to provide the type of meme (e.g., positive or negative meme) desired by the author to be associated with the particular designated primary object. Upon receiving a selection of one or more of these options, the document is modified accordingly to generate the desired meme in the document.",20160217,IBM
47,Characteristics of a speaker are estimated using speech processing and machine learning. The characteristics of the speaker are used to automatically customize a user interface of a client device for the speaker.,20160808,Google
48,"A computer system provides an automated tool that processes audio data and data about computer operating context to automatically capture information, such as information from conversations and meetings. The computer operating context can include, for example, environmental data sensed, or other contextual information maintained or tracked, by the computer. The audio data and computer operating context are processed by the computer to detect starting conditions for data extraction. After detecting a starting condition, the processor initiates speech processing on the audio information to generate corresponding text data. The processor processes the text data using text filters that define salient patterns in the text data. Such salient patterns can correspond to, for example, tasks to be performed, follow-up appointments and other events, messages to be sent, important points or notes, and the like. Such automatic processing improves user productivity in tracking valuable information from meetings and other conversations.",20150301,Microsoft
49,"One or more systems and/or techniques are provided for automatic closed captioning for media content. In an example, real-time content, occurring within a threshold timespan of a broadcast of media content (e.g., social network posts occurring during and an hour before a live broadcast of an interview), may be accessed. A list of named entities, occurring within the social network data, may be generated (e.g., Interviewer Jon, Interviewee Kathy, Husband Dave, Son Jack, etc.). A ranked list of named entities may be created based upon trending named entities within the list of named entities (e.g., a named entity may be ranked higher based upon a more frequent occurrence within the social network posts). A dynamic grammar (e.g., library, etc.) may be built based upon the ranked list of named entities. Speech recognition may be performed upon the broadcast of media content utilizing the dynamic grammar to create closed caption text.",20150602,Microsoft
50,"A display apparatus and a method for questions and answers includes a display unit includes an input unit configured to receive user's speech voice; a communication unit configured to perform data communication with an answer server; and a processor configured to create and display one or more question sentences using the speech voice in response to the speech voice being a word speech, create a question language corresponding to the question sentence selected from among the displayed one or more question sentences, transmit the created question language to the answer server via the communication unit, and, in response to one or more answer results related to the question language being received from the answer server, display the received one or more answer results. Accordingly, the display apparatus may provide an answer result appropriate to a user's question intention although a non-sentence speech is input.",20151022,SAMSUNG
51,"A speech-to-speech (S2S) translation system may utilize a damaging channel model to adapt machine translation (MT) training data so that a MT engine of the S2S translation system that is trained with the adapted training data can make better use of output received from an automated speech recognition (ASR) engine of the S2S translation system. The S2S translation system may include a MT training module that uses MT technology in order to simulate a particular ASR engine output by treating the ASR engine as a “noisy channel”. A process may include modeling ASR errors of a particular ASR engine based at least in part on output of the ASR engine to create an ASR simulation model, and performing machine translation to generate training data based at least in part on the ASR simulation model. The MT engine of the S2S translation system may then be trained using the generated training data.",20141124,Microsoft
52,Embodiments relate to facilitating a meeting. A method for facilitating a meeting of a group of participants is provided. The method generates a graph of words from speeches of the participants as the words are received from the participants. The method partitions the group of participants into a plurality of subgroups of participants. The method performs a graphical text analysis on the graph to identify a cognitive state for each participant and a cognitive state for each subgroup of participants. The method informs at least one of the participants about the identified cognitive state of a participant or a subgroup of participants.,20141103,IBM
53,"A method of operating an electronic device is provided, the method including: receiving, by the electronic device that includes a display and a voice receiving device, a sequence of speech elements through the voice receiving device; displaying, on the display by the electronic device, first information that is based on at least a part of a first speech element out of the speech elements; and displaying, on the display by the electronic device, second information, which is different than the first information and is based on at least a part of a second speech element that is received later than the first speech element among the speech elements.",20151014,SAMSUNG
54,"Computer-implemented techniques can include receiving a selected word in a source language, obtaining one or more parts of speech for the selected word, and for each of the one or more parts-of-speech, obtaining candidate translations of the selected word to a different target language, each candidate translation corresponding to a particular semantic meaning of the selected word. The techniques can include for each semantic meaning of the selected word: obtaining an image corresponding to the semantic meaning of the selected word, and compiling translation information including (i) the semantic meaning, (ii) a corresponding part-of-speech, (iii) the image, and (iv) at least one corresponding candidate translation. The techniques can also include outputting the translation information.",20150518,Google
55,"The technology described herein can improve the operation of a computerized text entry system (e.g., keyboard, speech to text) by making grammatically correct auto-complete suggestions as a user enters text. The technology described herein builds and uses a set of generalized rules that make the auto-complete feature sensitive to the context of what has already been typed, particularly at the level of a sentence or phrase. The technology described herein receives one or more words within a partially completed sentence and outputs one or more contrastive grammatical categories that the next word may be if the final sentence is to be grammatical.",20150616,Microsoft
56,"In a computer system for navigating to a location in recorded content, a computer receives a descriptive term or phrase associated with a searchable tag. The searchable tag corresponds to a point-in-time at which a non-speech sound occurred during the recording of recorded content of a communication between a plurality of participants. The recorded content includes speech from one or more of the plurality of participants, the descriptive term includes an automatically generated phonetic translation of the non-speech sound, and the non-speech sound was transmitted to the plurality of participants during the recording. The computer navigates to a location in the recorded content corresponding to the point-in-time at which the non-speech sound occurred.",20160727,IBM
57,"Systems and methods are utilized for recognizing speech that is partially in a foreign language. The systems and methods receive speech input from a user and detect if a rule or sentence entry grammar structure utilizing a foreign word has been uttered. To recognize the foreign word, a foreign word grammar is utilized. The foreign word grammar includes rules for recognizing the uttered foreign word. Two rules may be included in the foreign word grammar for each legitimate or slang term included in the foreign word grammar A first rule corresponds to the spoken form of the foreign word, and the second rule corresponds to the spelling form of the foreign word. The foreign word grammar may also utilize a prefix tree. Upon recognizing the foreign word, the recognized foreign word may be sent to an application to retrieve the pronunciation, translation, or definition of the foreign word.",20140717,Microsoft
58,"Predicting and learning users' intended actions on an electronic device based on free-form speech input. Users' actions can be monitored to develop a list of carrier phrases having one or more actions that correspond to the carrier phrases. A user can speak a command into a device to initiate an action. The spoken command can be parsed and compared to a list of carrier phrases. If the spoken command matches one of the known carrier phrases, the corresponding action(s) can be presented to the user for selection. If the spoken command does not match one of the known carrier phrases, search results (e.g., Internet search results) corresponding to the spoken command can be presented to the user. The actions of the user in response to the presented action(s) and/or the search results can be monitored to update the list of carrier phrases.",20160705,Google
59,"A grammar correcting method is provided, the method including receiving a sentence generated based on speech recognition, receiving information associated with a speech recognition result of the sentence and correcting grammar in the sentence based on the information associated with the speech recognition results of the sentence.",20150625,SAMSUNG
60,"According to one embodiment, an interpretation apparatus includes a translator, a calculator and a generator. The translator performs machine translation on a speech recognition result corresponding to an input speech audio from a first language into a second language to generate a machine translation result. The calculator calculates a word number based on a first time when the machine translation result is generated and a second time when output relating to a prior machine translation result generated prior to the machine translation result ends, the word number being 0 or larger. The generator omits at least the word number of words from the machine translation result to generate an abridged sentence output while being associated with the speech audio.",20160309,TOSHIBA
61,"Technologies are described herein for cross-language speech recognition and translation. An example method of speech recognition and translation includes receiving an input utterance in a first language, the input utterance having at least one name of a named entity included therein and being pronounced in a second language, utilizing a customized language model to process at least a portion of the input utterance, and identifying the at least one name of the named entity from the input utterance utilizing a phonetic representation of the at least one name of the named entity. The phonetic representation has a pronunciation of the at least one name in the second language.",20150515,Microsoft
62,"According to one embodiment, a presentation support apparatus includes a switcher, an acquirer, a recognizer and a controller. The switcher switches a first content to a second content in accordance with an instruction of a first user, the first content and the second content being presented to the first user. The acquirer acquires a speech related to the first content from the first user as a first audio signal. The recognizer performs speech recognition on the first audio signal to obtain a speech recognition result. The controller controls continuous output the first content to a second user, when the first content is switched to the second content, during a first period after presenting the speech recognition result to the second user.",20160309,TOSHIBA
63,"According to an embodiment, a device includes a table creator, an estimator, and a dictionary creator. The table creator is configured to create a table based on similarity between distributions of nodes of speech synthesis dictionaries of a specific speaker in respective first and second languages. The estimator is configured to estimate a matrix to transform the speech synthesis dictionary of the specific speaker in the first language to a speech synthesis dictionary of a target speaker in the first language, based on speech and a recorded text of the target speaker in the first language and the speech synthesis dictionary of the specific speaker in the first language. The dictionary creator is configured to create a speech synthesis dictionary of the target speaker in the second language, based on the table, the matrix, and the speech synthesis dictionary of the specific speaker in the second language.",20150709,TOSHIBA
64,"According to one embodiment, a machine translation apparatus includes a speech recognition unit that receives a speech input of a source language, recognizes the speech input of the source language, and generates a text of the source language, the speech input of the source language being sequentially-input, the text of the source language being the results of a speech recognition and an analysis information; a dividing unit that that decides a dividing position of units to be processed and information of order to be translated, based on the analysis information, the units to be processed being semantic units, each of the semantic units representing a partial meaning of the text of the source language; a machine translation unit that sequentially translates the units to be processed into a target language; a translation control unit that arranges the translated units based on the information of order to be translated and generates a text of the target language; and an output unit that outputs the text of the target language.",20150914,TOSHIBA
65,"Methods and systems are provided for discriminating ambiguous expressions to enhance user experience. For example, a natural language expression may be received by a speech recognition component. The natural language expression may include at least one of words, terms, and phrases of text. A dialog hypothesis set from the natural language expression may be created by using contextual information. In some cases, the dialog hypothesis set has at least two dialog hypotheses. A plurality of dialog responses may be generated for the dialog hypothesis set. The dialog hypothesis set may be ranked based on an analysis of the plurality of the dialog responses. An action may be performed based on ranking the dialog hypothesis set.",20141230,Microsoft
66,"Systems for associating audio files with cells of a spreadsheet are provided. Both audio files and data may be associated with a single cell of the spreadsheet. An audio file may be recorded, retrieved from storage, or converted from a document (e.g., using text-to-speech technology) for association with a spreadsheet. Upon association, audio parameters may be viewed and/or manipulated by a user, providing audio processing functionality within a spreadsheet. Controls may be provided for listening to the audio file and/or playing the audio file in response to spreadsheet data satisfying a condition. Text transcriptions (e.g., speech-to-text) of audio files may be inserted into the spreadsheet. For low vision users, audio transcriptions (e.g., text-to-speech) of data may be generated and “played” for the user. Spreadsheet operations (e.g., sort and/or filter operations) may also be performed on a range of cells based on audio parameters of associated audio files.",20160630,Microsoft
67,"A system can be configured to perform tasks such as converting recorded speech to a sequence of phonemes that represent the speech, converting an input sequence of graphemes into a target sequence of phonemes, translating an input sequence of words in one language into a corresponding sequence of words in another language, or predicting a target sequence of words that follow an input sequence of words in a language (e.g., a language model). In a speech recognizer, the RNN system may be used to convert speech to a target sequence of phonemes in real-time so that a transcription of the speech can be generated and presented to a user, even before the user has completed uttering the entire speech input.",20161111,Google
68,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20170530,Google
69,A speech recognition apparatus includes a probability calculator configured to calculate phoneme probabilities of an audio signal using an acoustic model; a candidate set extractor configured to extract a candidate set from a recognition target list; and a result returner configured to return a recognition result of the audio signal based on the calculated phoneme probabilities and the extracted candidate set.,20160427,SAMSUNG
70,"According to one embodiment, a speech translation apparatus includes a speech recognition unit, a machine translation unit, an extracting unit, and a receiving unit. The extracting unit extracts words used for a meeting from a word set, based on information related to the meeting, and sends the extracted words to the speech recognition unit and the machine translation unit. The receiving unit receives the speech in a first language in the meeting. The speech recognition unit recognizes the speech in the first language as a text in the first language. The machine translation unit translates the text in the first language into a text in a second language.",20160912,TOSHIBA
71,"An apparatus for notification of speech of interest to a user includes a voice analyzer configured to recognize speech, evaluate a relevance between a result of the speech recognition and a determined user's topic of interest, and determine whether to provide a notification; and an outputter configured to, in response to the voice analyzer determining to provide the notification, generate and output a notification message.",20160714,SAMSUNG
72,"Locations and presentation orders of objects of interest (e.g., speech bubbles) in digital graphic novel content are identified such that expanded versions of the objects of interest can be presented to a reader. Specifically, digital graphic novel content is received and locations of interest regions (e.g., rectangular text regions of speech bubbles) in the content are identified by applying a machine-learned model to the content. Locations and presentation orders of objects of interest in the digital graphic novel content are identified based on the identified locations of the interest regions. The digital graphic novel content and presentation metadata including the locations and presentation orders of the objects of interest are provided to a reading device such that expanded versions of the objects of interest are presented to the user in accordance with the presentation metadata.",20160617,Google
73,"Systems for performing operations on dynamic data associated with cells of a spreadsheet are provided. Both dynamic data and static data may be associated with a single cell of the spreadsheet. Upon association, parameters (including stream attributes, video attributes, image attributes and audio attributes) may be manipulated by a user, providing dynamic data processing functionality within a spreadsheet. Controls may be provided for playing the dynamic data and/or playing the dynamic data in response to spreadsheet data satisfying a condition. Operations for inserting text transcriptions (e.g., speech-to-text) of an audio track associated with dynamic data as subtitles may further be provided. Spreadsheet operations (e.g., sort and/or filter operations) may also be performed on a range of cells based on parameters of associated dynamic data.",20160930,Microsoft
74,"Contextual note taking is described. A note taking assistant can receive an indication of a specific presentation session. This indication can be used by the note taking assistant to access information or content related to the session. The note taking assistant can receive specific presentation session content, which includes identifiable context images. Identifiable context images are meant to define an individual page, an individual slide, or other atomic unit in the presentation. The note taking assistant operates by receiving a navigation message, changing the current assistant context image to a current presenter context image based on the navigation message; receiving a speech-to-text message comprising a unit of text; displaying the current presenter context image, and displaying the unit of text associated with the current presenter context image; and storing the unit of text associated with the current presenter context image.",20160307,Microsoft
75,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20161114,Google
76,"A method, executed by a computer, for displaying dynamic content in a group communication environment includes determining a topic of a communication session having multiple participants, searching repositories for content corresponding to the topic, determining the participants' identities, retrieving the content, and enabling the participants to view the content. In some embodiments, the content is an image containing a face of a participant, and a speech indicator is displayed at the face of each participant as they comment in the communication session. In some embodiments, the relationship between the topic and the content is predefined. The topic of the communication session may be determined using natural language processing techniques. A computer program product and computer system corresponding to the above method are also disclosed herein.",20150729,IBM
77,"Systems for associating audio files with cells of a spreadsheet are provided. Both audio files and data may be associated with a single cell of the spreadsheet. An audio file may be recorded, retrieved from storage, or converted from a document (e.g., using text-to-speech technology) for association with a spreadsheet. Upon association, audio parameters may be viewed and/or manipulated by a user, providing audio processing functionality within a spreadsheet. Controls may be provided for listening to the audio file and/or playing the audio file in response to spreadsheet data satisfying a condition. Text transcriptions (e.g., speech-to-text) of audio files may be inserted into the spreadsheet. For low vision users, audio transcriptions (e.g., text-to-speech) of data may be generated and “played” for the user. Spreadsheet operations (e.g., sort and/or filter operations) may also be performed on a range of cells based on audio parameters of associated audio files.",20160630,Microsoft
78,"Speech generating devices, communication systems, and methods for communicating using the devices and systems are disclosed herein. In certain examples, a communication system is configured to receive a generated communication, establish a connection between a speech generating device and a computing device subsequent to receipt of the generated communication, and transmit the generated communication to the computing device. In other examples, a computing device is configured to establish a connection with a speech generating device, and receive a transmission generated by the speech generating device following the connection, the transmission including previously generated communications or real-time communication segments or proxies. In other examples, a speech generating device is configured to establish a connection with one or more computing devices, receive one or more suggestions from at least one computing device during generation of the communication, and display a suggestion on the display device as a shortcut input key.",20151009,Microsoft
79,"Systems, methods, and apparatus for accessing distributed models in automated machine processing, including using large language models in machine translation, speech recognition and other applications.",20170406,Google
80,"According to one embodiment, an interaction apparatus includes an acquirer, an estimator, an extractor, a selector and a controller. The acquirer acquires a text describing an intention of a user. The estimator estimates the intention from the text. The extractor extracts a keyword from the text. The selector selects a word having a part of speech from the text if the keyword having an attribute does not exist in the text when the keyword is to be assigned to a slot, the slot including information relating to the attribute and part of speech of a word necessary to execute a service corresponding to the intention. The controller assigns the selected word to the slot.",20161221,TOSHIBA
81,"A user activity pattern may be ascertained using signal data from a set of computing devices. The activity pattern may be used to infer user intent with regards to a user interaction with a computing device or to predict a likely future action by the user. In one implementation, a set of computing devices is monitored to detect user activities using sensors associated with the computing devices. Activity features associated with the detected user activities are determined and used to identify an activity pattern based on a plurality of user activities having similar features. Examples of user activity patterns may include patterns based on time, location, content, or other context. The inferred user intent or predicted future actions may be used to facilitate understanding user speech or determining a semantic understanding of the user.",20160331,Microsoft
82,"In some implementations, a language proficiency of a user of a client device is determined by one or more computers. The one or more computers then determines a text segment for output by a text-to-speech module based on the determined language proficiency of the user. After determining the text segment for output, the one or more computers generates audio data including a synthesized utterance of the text segment. The audio data including the synthesized utterance of the text segment is then provided to the client device for output.",20170719,Google
83,Embodiments relate to facilitating a meeting. A method for facilitating a meeting of a group of participants is provided. The method generates a graph of words from speeches of the participants as the words are received from the participants. The method partitions the group of participants into a plurality of subgroups of participants. The method performs a graphical text analysis on the graph to identify a cognitive state for each participant and a cognitive state for each subgroup of participants. The method informs at least one of the participants about the identified cognitive state of a participant or a subgroup of participants.,20161219,IBM
84,"Techniques, systems, and devices for managing streaming media among end user devices in a video conferencing system are described. For example, a transcript may be automatically generated for a video conference. In one example, a method may include receiving a combined media stream comprising a plurality of media sub-streams each associated with one of a plurality of end user devices, wherein each of the plurality of media sub-streams comprises a respective video component and a respective audio component. The method may also include, for each of the media-sub-streams, separating the audio component from the respective video component, for each audio component of the respective media sub-streams, transcribing speech from the audio component to text for the respective media sub-stream, and combining the text for each of the respective media sub-streams into a combined transcription. In some examples, the combined transcription may also be translated into a user selected language.",20160912,Google
85,"A portion of speech is captured when spoken by a near-end user. A near-end user terminal conducts a communication session, over a network, between the near-end user and one or more far-end users, the session including a message sent to the one or more far-end users. A vetting mechanism is provided via a touchscreen user interface of the near-end user terminal, to allow the near-end user to vet an estimated transcription of the portion of speech prior to being sent to the one or more far-end users in the message. According to the vetting mechanism: (i) a first gesture performed by the near-end user through the touchscreen user interface accepts the estimated transcription to be included in a predetermined role in the sent message, whilst (ii) one or more second gestures performed by the near-end user through the touchscreen user interface each reject the estimated transcription to be sent in the message.",20150918,Microsoft
86,"In some implementations, a language proficiency of a user of a client device is determined by one or more computers. The one or more computers then determines a text segment for output by a text-to-speech module based on the determined language proficiency of the user. After determining the text segment for output, the one or more computers generates audio data including a synthesized utterance of the text segment. The audio data including the synthesized utterance of the text segment is then provided to the client device for output.",20170403,Google
87,"Methods and systems for language processing includes training one or more automatic speech recognition models using an automatic speech recognition dictionary. A set of N automatic speech recognition hypotheses for an input is determined, based on the one or more automatic speech recognition models, using a processor. A best hypothesis is selected using a discriminative language model and a list of relevant words. Natural language processing is performed on the best hypothesis.",20151214,IBM
88,"Responsive to determining to transition a voice call from voice communications over a voice network to streamed text over a packetized data network, a voice conversation correlation identifier is created that identifies the voice call and specifies incoming and outgoing streamed text data as part of the voice call. Additional outgoing speech spoken by a user associated with the voice call is converted to streamed text data. The streamed text data identified by the voice conversation correlation identifier is sent within an outgoing text stream over the packetized data network. Streamed response text data identified by the voice conversation correlation identifier is received within an incoming text stream over the packetized data network. The received streamed response text data within the incoming text stream is converted to speech output as part of the voice call.",20150916,IBM
89,"The personal translator implementations described herein provide a speech translation device that pairs with a computing device to translate in-person conversations. The speech translation device can be wearable. In one implementation the personal translator comprises a speech translation device with at least one microphone that captures input signals representing nearby speech of a first user/wearer of the device and at least one other nearby person in a conversation in two languages; a wireless communication unit that sends the captured input signals representing speech to a nearby computing device, and receives for each language in the conversation, language translations from the computing device; and at least one loudspeaker that outputs the language translations to the first user/wearer and at least one other nearby person. The language translations in text form can be displayed on a display at the same time the language translations are output to the loudspeaker(s).",20150824,Microsoft
90,"Embodiments provide a system and method for short form and long form detection. Using a language-independent process, the detection system can ingest a corpus of documents, pre-process those documents by tokenizing the documents and performing a part-of-speech analysis, and can filter one or more candidate short forms using one or more filters that select for semantic criteria. Semantic criteria can include the part of speech of a token, whether the token contains more than a pre-determined amount of symbols or digits, whether the token appears too frequently in the corpus of documents, and whether the token has at least one uppercase letter. The detection system can detect short forms independent of case and punctuation, and independent of language-specific metaphone variants.",20160628,IBM
91,"The disclosed subject matter provides a system, computer readable storage medium, and a method providing an audio and textual transcript of a communication. A conferencing services may receive audio or audio visual signals from a plurality of different devices that receive voice communications from participants in a communication, such as a chat or teleconference. The audio signals representing voice (speech) communications input into respective different devices by the participants. A translation services server may receive over a separate communication channel the audio signals for translation into a second language. As managed by the translation services server, the audio signals may be converted into textual data. The textual data may be translated into text of different languages based the language preferences of the end user devices in the teleconference. The translated text may be further translated into audio signals.",20170731,Google
92,"According to one embodiment, a machine translation apparatus includes a memory and a hardware processor in electrical communication with the memory. The memory stores instructions. The processor execute the instructions to translate a text in a first language to a plurality of translation results in a second language, output at least one of the plurality of translation results to a screen, and synthesize a speech from at least another one of the plurality of translation results.",20160906,TOSHIBA
93,"In an embodiment a device to convert conversations from a meeting to text and annotate the text is disclosed. In an embodiment a device is disclosed, comprising: a microphone; a camera; a processor; and a storage comprising a set of instructions; wherein the set of instructions causes a processor to: receive from the microphone, an audio recording containing speech of a participant of a meeting; receive from the camera, a video of the participant; identify the participant; convert the speech of the participant to a digital text; develop a skeletal map of the participant; recognize a gesture of the participant from the skeletal maps; detect and identify a target of the gesture; based on the target and the gesture determine an annotation for the digital text corresponding to a point of time of the gesture.",20150826,Microsoft
94,"A speech recognition apparatus includes a predictor configured to predict a word class of a word following a word sequence that has been previously searched for based on the word sequence that has been previously searched for; and a decoder configured to search for a candidate word corresponding to a speech signal, extend the word sequence that has been previously searched for using the candidate word that has been searched for, and adjust a probability value of the extended word sequence based on the predicted word class.",20161021,SAMSUNG
95,"An approach for augmenting a teleconference based on cognitive computing. A teleconference transcript is created based on speech-to-text conversion. A portion of the teleconference transcript is replaced based on a substitute word store and on cognitive computing text substitution. A second teleconference transcript and text-to-speech conversion creates augmented real-time audio. Participant audio and augmented audios are combined to create and output augmented audio to teleconference participants. Audio augmentation includes noise reduction, volume normalization, content change and natural language translation.",20150914,IBM
96,"Systems for associating audio files with cells of a spreadsheet are provided. Both audio files and data may be associated with a single cell of the spreadsheet. An audio file may be recorded, retrieved from storage, or converted from a document (e.g., using text-to-speech technology) for association with a spreadsheet. Upon association, audio parameters may be viewed and/or manipulated by a user, providing audio processing functionality within a spreadsheet. Controls may be provided for listening to the audio file and/or playing the audio file in response to spreadsheet data satisfying a condition. Text transcriptions (e.g., speech-to-text) of audio files may be inserted into the spreadsheet. For low vision users, audio transcriptions (e.g., text-to-speech) of data may be generated and “played” for the user. Spreadsheet operations (e.g., sort and/or filter operations) may also be performed on a range of cells based on audio parameters of associated audio files.",20160630,Microsoft
97,A language translation application on a user device includes a user interface that provides relevant textual and graphical feedback mechanisms associated with various states of voice input and translated speech.,20170316,Google
98,A method and apparatus is provided for annotating video content with metadata generated using speech recognition technology. The method begins by rendering video content on a display device. A segment of speech is received from a user such that the speech segment annotates a portion of the video content currently being rendered. The speech segment is converted to a text-segment and the text-segment is associated with the rendered portion of the video content. The text segment is stored in a selectively retrievable manner so that it is associated with the rendered portion of the video content.,20170328,Google
99,"Systems for associating videos with cells of a spreadsheet are provided. Both dynamic data and static data may be associated with a single cell of the spreadsheet. Upon association, parameters (including video attributes, image attributes and audio attributes) may be viewed and/or manipulated by a user, providing video and audio processing functionality within a spreadsheet. Controls may be provided for playing the video and/or playing the video in response to spreadsheet data satisfying a condition. Text transcriptions (e.g., speech-to-text) of an audio track associated with a video file may be inserted as subtitles into the video or a plurality of individual frames for the video. Spreadsheet operations (e.g., sort and/or filter operations) may also be performed on a range of cells based on parameters of an associated video.",20160930,Microsoft
100,"According to an embodiment, a conference support apparatus includes a recognizer, a detector, a summarizer, and a subtitle generator. The recognizer is configured to recognize speech in speech data and generate text data. The detector is configured to detect a correction operation on the text data, the correction operation being an operation of correcting character data that has been incorrectly converted. The summarizer is configured to generate a summary relating to the text data subsequent to a part to which the correction operation is being performed, among the text data, when the correction operation is being detected. The subtitle generator is configured to generate subtitle information corresponding to the summary when the correction operation is being detected, and configured to generate subtitle information corresponding to the text data except when the correction operation is being detected.",20161221,TOSHIBA
101,"An electronic device is provided. The electronic device includes at least one communication circuit, a display, a speaker, a memory, and a processor electrically connected to the communication circuit, the display, the memory and the speaker. The processor is configured to receive a message that includes one or more items of a link or content through the at least one communication circuit, parse the message in order to recognize the one or more items, extract or receive content from the one or more items or from an external resource related to the one or more items, convert the message into at least one of a speech, a sound, an image, a video, and data according to at least one of the parsed message and the extracted or received content, and provide at least one of the speech, the sound, the image, the video, and the data to the speaker or the at least one communication circuit.",20170221,SAMSUNG
102,"A technique for generating a new equivalent phrase for an input phrase includes receiving a first input phrase for natural language expansion. Tokens that correspond to parts of speech are generated for the first input phrase. An original grammar tree is generated using at least some of the tokens. One or more alternate grammar trees are generated that are different from the original grammar tree but substantially equivalent to the original grammar tree. One or more synonyms for at least one of the tokens are generated. Finally, one or more new phrases are generated based on the one or more alternate grammar trees and the one or more synonyms.",20150925,IBM
103,"In some implementations, a language proficiency of a user of a client device is determined by one or more computers. The one or more computers then determines a text segment for output by a text-to-speech module based on the determined language proficiency of the user. After determining the text segment for output, the one or more computers generates audio data including a synthesized utterance of the text segment. The audio data including the synthesized utterance of the text segment is then provided to the client device for output.",20160128,Google
104,"A content server accessing an audio stream, and inputs portions of the audio stream into one or more non-speech classifiers for classification, the non-speech classifiers generating, for portions of the audio stream, a set of raw scores representing likelihoods that the respective portion of the audio stream includes an occurrence of a particular class of non-speech sounds associated with each of the non-speech classifiers. The content server generates binary scores for the sets of raw scores, the binary scores generated based on a smoothing of a respective set of raw scores. The content server applies a set of non-speech captions to portions of the audio stream in time, each of the sets of non-speech captions based on a different one of the set binary scores of the corresponding portion of the audio stream.",20160823,Google
105,"Techniques (300, 400, 500) and apparatuses (100, 200, 700) for recognizing accented speech are described. In some embodiments, an accent module recognizes accented speech using an accent library based on device data, uses different speech recognition correction levels based on an application field into which recognized words are set to be provided, or updates an accent library based on corrections made to incorrectly recognized speech.",20170321,Google
106,"According to one embodiment, a classification apparatus includes the following elements. The target log extraction unit extracts a set of dialogue logs for a dialogue from dialogue logs for dialogues between a user and an interactive system, the dialogue including a first speech and a second speech of the user, the set of dialogue logs including information indicative of a classification used for a search performed by the interactive system based on the first speech, information indicative of a failure in the first search, and data acquired as a result of a search performed by the interactive system based on the second speech. The classification relation generator generates a classification relation in which the classification is associated with the data.",20170215,TOSHIBA
107,"A computer system provides an automated tool that processes audio data and data about computer operating context to automatically capture information, such as information from conversations and meetings. The computer operating context can include, for example, environmental data sensed, or other contextual information maintained or tracked, by the computer. The audio data and computer operating context are processed by the computer to detect starting conditions for data extraction. After detecting a starting condition, the processor initiates speech processing on the audio information to generate corresponding text data. The processor processes the text data using text filters that define salient patterns in the text data. Such salient patterns can correspond to, for example, tasks to be performed, follow-up appointments and other events, messages to be sent, important points or notes, and the like. Such automatic processing improves user productivity in tracking valuable information from meetings and other conversations.",20170531,Microsoft
108,"An apparatus for correcting a character string in a text of an embodiment includes a first converter, a first output unit, a second converter, an estimation unit, and a second output unit. The first converter recognizes a first speech of a first speaker, and converts the first speech to a first text. The first output unit outputs a first caption image indicating the first text. The second converter recognizes a second speech of a second speaker for correcting a character string to be corrected in the first text, and converts the second speech to a second text. The estimation unit estimates the character string to be corrected, based on text matching between the first text and the second text. The second output unit outputs a second caption image indicating that the character string to be corrected is to be replaced with the second text.",20161221,TOSHIBA
109,"Techniques and systems to provide speech recognition services over a network using a standard interface are described. In an embodiment, a technique includes accepting a speech recognition request that includes at least audio input, via an application program interface (API). The speech recognition request may also include additional parameters. The technique further includes performing speech recognition on the audio according to the request and any specified parameters; and returning a speech recognition result as a hypertext protocol (HTTP) response. Other embodiments are described and claimed.",20161219,Microsoft
110,Techniques and architectures may be used to generate and perform a process using weighted finite-state transducers involving generic input search graphs. The process need not pursue theoretical optimality and instead search graphs may be optimized without an a priori optimization step. The process may result in an automatic speech recognition (ASR) decoder that is substantially faster than ASR decoders the include the optimization step.,20160601,Microsoft
111,"Free-form text in a document can be analyzed using natural-language processing to determine actionable items specified by users in the text or to provide recommendations, e.g., by automatically analyzing texts from multiple users. Words or phrases of the text can be mapped to classes of a model. An actionable item can be determined using the mapped words or phrases that match a selected grammar pattern. Items can be ranked, e.g., based on frequency across multiple documents. In some examples, the classes can include a suggestion-indicator class or a modal-indicator class, and the selected grammar pattern can include one of those classes. In some examples, the mapping can use a dictionary. A new term not in the dictionary can be automatically associated with classes based on attributes of the new term and of terms in the dictionary, e.g., the new term's part of speech or neighboring terms.",20150630,Microsoft
112,"An “Utterance-Based Knowledge Tool” monitors user utterances (e.g., user speech or text inputs) to identify relevant statements of facts in declarative utterances of a user. A semantic parser is applied to each statement of facts to parse assertions comprising instances of two or more entities and relations between those entities. As such, each assertion explicitly delimits a relation between two particular entities (one of which may be the user) that are relevant to the particular user. The Utterance-Based Knowledge Tool places or categorizes the identified assertions (which each include entities and relations) into one or more of a plurality of predefined classes. These classified assertions are then applied to construct and/or update a personal knowledge graph for the user. This personal knowledge graph is then applied to respond to user queries, thereby improving personal relevancy of query responses provided to the user.",20150726,Microsoft
113,"Embodiments provide a system and method for short form and long form detection. Using a language-independent process, the detection system can ingest a corpus of documents, pre-process those documents by tokenizing the documents and performing a part-of-speech analysis, and can filter one or more candidate short forms using one or more filters that select for semantic criteria. Semantic criteria can include the part of speech of a token, whether the token contains more than a pre-determined amount of symbols or digits, whether the token appears too frequently in the corpus of documents, and whether the token has at least one uppercase letter. The detection system can detect short forms independent of case and punctuation, and independent of language-specific metaphone variants.",20180629,IBM
114,"Generation of expressive content is provided. An expressive synthesized speech system provides improved voice authoring user interfaces by which a user is enabled to efficiently author content for generating expressive output. An expressive synthesized speech system provides an expressive keyboard for enabling input of textual content and for selecting expressive operators, such as emoji objects or punctuation objects for applying predetermined prosody attributes or visual effects to the textual content. A voicesetting editor mode enables the user to author and adjust particular prosody attributes associated with the content for composing carefully-crafted synthetic speech. An active listening mode (ALM) is provided, which when selected, a set of ALM effect options are displayed, wherein each option is associated with a particular sound effect and/or visual effect. The user is enabled to rapidly respond with expressive vocal sound effects or visual effects while listening to others speak.",20161109,Microsoft
115,"In an aspect of the present disclosure, a method for providing an alternate modality of input for filling a form field in response to a failure of voice recognition is disclosed including prompting the user for information corresponding to a field of a form, generating speech data by capturing a spoken response of the user to the prompt using at least one input device, attempting to convert the speech data to text, determining that the attempted conversion has failed, evaluating the failure using at least one speech rule, selecting, based on the evaluation, an alternate input modality to be used for receiving the information corresponding to the field of the form, receiving the information corresponding to the field of the form from the alternate input modality, and injecting the received information into the field of the form.",20170201,IBM
116,"To address the issues of handling conversations with multiple users, an intelligent digital assistant system is provided. The system may include at least one microphone configured to receive an audio input, a speaker configured to emit an audio output, and a processor. The processor may be configured engage in a conversation with a first user, and, concurrent with the first user being engaged in the conversation with the system, recognize speech of one or more additional users in the audio input. The processor may process the recognized speech of the one or more additional users to determine a context for each additional user, and execute a conversation disentanglement module to select and perform one or more predetermined conversation disentanglement actions according to the context of the recognized speech of each additional user.",20170630,Microsoft
117,"Methods, computer program products, and systems are presented. The method computer program products, and systems can include, for instance: obtaining speech based message data and biometric data of a speaker user of a messaging system, the speech based message data being input into a computer device by the speaker user and the biometric data indicating one or more aspect of a physical condition of the speaker user during the input of the speech based message data into the computer device; processing data to determine a truthfulness parameter of the speech based message data, the processing data to determine a truthfulness parameter including processing the biometric data, the truthfulness parameter indicating a probability that the speech based message data is knowingly untruthful; and associating the truthfulness parameter to the speech based message data, wherein the associating includes tagging the speech based message data with the truthfulness parameter.",20170404,IBM
118,"A system is provided that employs a statistical approach to semi-supervised speech enhancement with a low-order non-negative matrix factorization (“NMF”). The system enhances noisy speech based on multiple dictionaries with dictionary atoms derived from the same clean speech samples and generates an enhanced speech representation of the noisy speech by combining, for each dictionary, a clean speech representation of the noisy speech generated based on a NMF using the dictionary atoms of the dictionary. The system generates frequency-domain (“FD”) clean speech sample representations of the clean speech samples, for example, using a Fourier transform. To generate each dictionary, the system generates a dictionary-unique initialization of the dictionary atoms and the activations and performs a NMF of the FD clean speech samples.",20170616,Microsoft
119,"Devices, systems, and methods for automatically creating a document. In one example, the system and method perform or include capturing, with a web-extension associated with a word-processing application, implicitly-tagged-content and an explicitly-tagged-content displayed on a web browser along with tags associated with the implicitly-tagged-content and the explicitly-tagged-content; receiving, with a speech-to-text interface, natural-language audio instruction associated with generating a document; generating, with a natural-language processor, a plain-text command associated with the natural-language audio instruction; retrieving personalized-content based on the plain-text command; and organizing, with a content-organizer, the personalized-content based on one or more criteria selected from a group consisting of page rank of a content displayed on the web browser, a source of the content, an authoring-style, and a document template. The document is generated using information received from the content-organizer.",20170327,Microsoft
120,"Techniques and systems to provide speech recognition services over a network using a standard interface are described. In an embodiment, a technique includes accepting a speech recognition request that includes at least audio input, via an application program interface (API). The speech recognition request may also include additional parameters. The technique further includes performing speech recognition on the audio according to the request and any specified parameters; and returning a speech recognition result as a hypertext protocol (HTTP) response. Other embodiments are described and claimed.",20170928,Microsoft
121,"A method of updating a grammar model used during speech recognition includes obtaining a corpus including at least one word, obtaining the at least one word from the corpus, splitting the at least one obtained word into at least one segment, generating a hint for recombining the at least one segment into the at least one word, and updating the grammar model by using at least one segment comprising the hint.",20180312,SAMSUNG
122,"Methods and systems are provided for discriminating ambiguous expressions to enhance user experience. For example, a natural language expression may be received by a speech recognition component. The natural language expression may include at least one of words, terms, and phrases of text. A dialog hypothesis set from the natural language expression may be created by using contextual information. In some cases, the dialog hypothesis set has at least two dialog hypotheses. A plurality of dialog responses may be generated for the dialog hypothesis set. The dialog hypothesis set may be ranked based on an analysis of the plurality of the dialog responses. An action may be performed based on ranking the dialog hypothesis set.",20171204,Microsoft
123,An approach is provided that assists visually impaired users. The approach analyzes a document that is being utilized by the visually impaired user. The analysis derives a sensitivity of the document. A vocal characteristic corresponding to the derived sensitivity is retrieved. Text from the document is audibly read to the visually impaired user with a text to speech process that utilizes the retrieved vocal characteristic. The retrieved vocal characteristic conveys the derived sensitivity of the document to the visually impaired user.,20180616,IBM
124,"Techniques and systems are disclosed for context-dependent speech recognition. The techniques and systems described enable accurate recognition of speech by accessing sub-libraries associated with the context of the speech to be recognized. These techniques translate audible input into audio data at a smart device and determine context for the speech, such as location-based, temporal-based, recipient-based, and application based context. The smart device then accesses a context-dependent library to compare the audio data with phrase-associated translation data in one or more sub-libraries of the context-dependent library to determine a match. In this way, the techniques allow access to a large quantity of phrases while reducing incorrect matching of the audio data to translation data caused by organizing the phrases into context-dependent sub-libraries.",20170410,Microsoft
125,"A method, computer program product, and system includes a processor(s) to obtain, over a communications network, media comprising at least one audio file, The processor(s) determines that the audio file includes human speech and extract the human speech from the audio file. The processor(s) contextualizes general elements of the human speech, based on analyzing metadata of the file. The processor(s) generates an unannotated textual representation of the human speech, where the unannotated textual representation includes spoken words. The processor(s) annotates the unannotated textual representation of the human speech, with indicators, where each indicator identifies a granular contextual element in the unannotated textual representation of the human speech. The processor(s) generates a textual representation of the human speech, by applying a template to the annotated textual representation, where the template defines values for the indicators in the annotated textual representation.",20170206,IBM
126,A language translation application on a user device includes a user interface that provides relevant textual and graphical feedback mechanisms associated with various states of voice input and translated speech.,20180508,Google
127,"A streams controller monitors multiple data streams with speech of a conversation with multiple speakers and uses text analytics and diaritization to identify speakers by their role in the conversation. Diaritization is applied to the audio of a data stream to associate a speaker with an SSRC from the data stream and a speaker identification (ID). The streams controller then runs text analytics on a text version of the speaker's speech to determine a speaker role for the speaker. The speaker role can be used to efficiently monitor conversations in a data stream to provide additional services. For example, speaker role can be used to analyze calls in a call center and provide services such as a transcript of conversations or enhanced customer support.",20170518,IBM
128,"A method, system and computer program product for assisting users to generate the desired meme in a document. A document is scanned to identify parts of speech, numerical text or numbers and images (collectively the “terms”) that provide positive/negative memes. A score for each of these terms is assigned. A score may then be assigned to each designated primary object (e.g., individual, company) discussed in the document using the scores assigned to the identified terms. Options may then be provided to modify the document to provide the type of meme (e.g., positive or negative meme) desired by the author to be associated with the particular designated primary object. Upon receiving a selection of one or more of these options, the document is modified accordingly to generate the desired meme in the document.",20180110,IBM
129,"A text editing apparatus includes: a display configured to display text; a user input unit configured to receive a speech signal for editing the text; and a controller configured to analyze a meaning of a word included in the speech signal, determine an editing target and an editing type, edit the text based on the determined editing target and editing type, and display the edited text on the display.",20160107,SAMSUNG
130,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20171221,Google
131,"A system can be configured to perform tasks such as converting recorded speech to a sequence of phonemes that represent the speech, converting an input sequence of graphemes into a target sequence of phonemes, translating an input sequence of words in one language into a corresponding sequence of words in another language, or predicting a target sequence of words that follow an input sequence of words in a language (e.g., a language model). In a speech recognizer, the RNN system may be used to convert speech to a target sequence of phonemes in real-time so that a transcription of the speech can be generated and presented to a user, even before the user has completed uttering the entire speech input.",20180806,Google
132,"A server includes a processor and memory, a network interface, and a first application executed by the processor and memory. The first application is configured to receive an input in a first language based on a call received via the network interface by a Voice over Internet Protocol (VoIP) application executed by the server. The call includes speech in a second language. The VoIP application includes speech recognition and translation functionality to process the call. The first application is configured to generate a response in the first language to the input. The first application is configured to transmit the response to the VoIP application to send a speech representation of the response in the second language via the call. The speech representation indicates quality of the speech recognition and translation functionality of the VoIP application.",20160816,Microsoft
133,An approach is provided that assists visually impaired users. The approach analyzes a document that is being utilized by the visually impaired user. The analysis derives a sensitivity of the document. A vocal characteristic corresponding to the derived sensitivity is retrieved. Text from the document is audibly read to the visually impaired user with a text to speech process that utilizes the retrieved vocal characteristic. The retrieved vocal characteristic conveys the derived sensitivity of the document to the visually impaired user.,20160822,IBM
134,"In an aspect of the present disclosure, a method for analyzing the speech delivery of a user is disclosed including presenting to the user a plurality of speech delivery analysis criteria, receiving from the user a selection of at least one of the speech delivery analysis criterion, receiving, from at least one sensing device, speech data captured by the at least one sensing device during the delivery of a speech by the user, transmitting the speech data and the selected at least one speech delivery analysis criterion to an analysis engine for analysis based on the selected at least one speech delivery analysis criteria, receiving, from the analysis engine an analysis report for the speech data, the analysis report comprising an analysis of the speech data performed by the analysis engine based on the selected at least one criterion, and presenting to the user the analysis report.",20170824,IBM
135,"Computer-implemented techniques can include receiving a selected word in a source language, obtaining one or more parts of speech for the selected word, and for each of the one or more parts-of-speech, obtaining candidate translations of the selected word to a different target language, each candidate translation corresponding to a particular semantic meaning of the selected word. The techniques can include for each semantic meaning of the selected word: obtaining an image corresponding to the semantic meaning of the selected word, and compiling translation information including (i) the semantic meaning, (ii) a corresponding part-of-speech, (iii) the image, and (iv) at least one corresponding candidate translation. The techniques can also include outputting the translation information.",20171222,Google
136,"According to an embodiment of the present invention, a computer-implemented method for modeling text-to-articulatory movement conversion is provided. In the method, text features are generated for a plurality of texts, wherein the text feature comprises a phoneme. Articulatory features are determined for the plurality of texts from a plurality of audio signals and a plurality of visual signals, wherein the audio signals record speeches of the plurality of texts, the visual signals record three-dimensional images of an articulator when speaking the plurality of texts, and the articulatory features indicate articulatory positions of the articulator in the speeches. A text-to-articulatory movement model is established with the text features as inputs and the articulatory features as outputs.",20161223,IBM
137,A method to execute computer-actionable directives conveyed in human speech comprises: receiving audio data recording speech from one or more speakers; converting the audio data into a linguistic representation of the recorded speech; detecting a target corresponding to the linguistic representation; committing to the data structure language data associated with the detected target and based on the linguistic representation; parsing the data structure to identify one or more of the computer-actionable directives; and submitting the one or more of the computer-actionable directives to the computer for processing.,20180611,Microsoft
138,"A streams controller monitors multiple data streams with speech of a conversation with multiple speakers and uses text analytics and diaritization to identify speakers by their role in the conversation. Diaritization is applied to the audio of a data stream to associate a speaker with an SSRC from the data stream and a speaker identification (ID). The streams controller then runs text analytics on a text version of the speaker's speech to determine a speaker role for the speaker. The speaker role can be used to efficiently monitor conversations in a data stream to provide additional services. For example, speaker role can be used to analyze calls in a call center and provide services such as a transcript of conversations or enhanced customer support.",20171108,IBM
139,"A dialogue system comprising:an input for receiving data relating to a speech or text signal originating from a user;an output for outputting information specified by an action; anda processor configured to:        update one or more system states based on the input data using one or more state tracker models, wherein the one or more system states comprise probability values associated with each of a plurality of possible values for each of a plurality of categories, wherein a category corresponds to a subject that the speech or text signal may relate to and can take on one or more values from a set of values;determine an action function and determine an action function input by inputting information generated using the system state and a set of stored information into a policy model, the set of stored information comprising a plurality of action functions;output information specified by the determined action function and the determined action function input at the output.",20170828,TOSHIBA
140,"Methods, systems, and apparatus, including computer programs encoded on computer storage media for speech recognition. One method includes obtaining an input acoustic sequence, the input acoustic sequence representing one or more utterances; processing the input acoustic sequence using a speech recognition model to generate a transcription of the input acoustic sequence, wherein the speech recognition model comprises a domain-specific language model; and providing the generated transcription of the input acoustic sequence as input to a domain-specific predictive model to generate structured text content that is derived from the transcription of the input acoustic sequence.",20161128,Google
141,"A method of translating a first language-based speech signal into a second language is provided. The method includes receiving the first language-based speech signal, converting the first language-based speech signal into a first language-based text including non-verbal information, by performing voice recognition on the first language-based speech signal, and translating the first language-based text into the second language, based on the non-verbal information.",20170925,SAMSUNG
142,"Systems, methods, and apparatus for accessing distributed models in automated machine processing, including using large language models in machine translation, speech recognition and other applications.",20180829,Google
143,"A system, computer program product, and method are provided for use with an intelligent computer platform to convert speech intents to one or more physical actions. The aspect of converting speech intent includes receiving audio, converting the audio to text, parsing the text into segments, identifying a physical action and associated application that are proximal in time to the received audio. A corpus is searched for evidence of a pattern associated with the received audio and corresponding physical action(s) and associated application. An outcome is generated from the evidence. The outcome includes identification of an application and production of an affirmative instruction. The instruction is converted to a user interface trace that is executed within the identified application.",20171025,IBM
144,"Training a machine by a machine learning technique for recognizing speech utterance to determine language fluency level of a user. Native speaker recorded data and language specific dictionary of heteronyms may be retrieved. The native speaker recorded data may be parsed and the heteronyms from the native speaker recorded data may be isolated. Linguistic features from the native speaker recorded data including at least linguistic features associated with the heteronyms may be extracted, and a language dependent machine learning model is generated based on the linguistic features.",20170905,IBM
145,"An auxiliary device charging case is used to facilitate translation features of a mobile computing device or auxiliary device. A first user, who may be a foreign language speaker, holds the charging case and speaks into the charging case. The charging case communicates the received speech to the mobile computing device, either directly or through the auxiliary device, which translates the received speech into a second language for a second user, who is the owner of the mobile computing device and auxiliary device. The second user may provide input in the second language, such as by speaking or typing into the auxiliary or mobile computing device. The mobile computing device may translate this second input to the first language, and transmit the translated input to the charging case either directly or through the auxiliary device. The charging case may output the translated second input to the first user, such as through a speaker or display screen.",20190102,Google
146,A method and an electronic device for translating a speech signal between a first language and a second language with minimized translation delay by translating fewer than all words of the speech signal according to a level of understanding of the second language by a user that receives the translation.,20181018,SAMSUNG
147,"Prosodic features are used for discriminating computer-directed speech from human-directed speech. Statistics and models describing energy/intensity patterns over time, speech/pause distributions, pitch patterns, vocal effort features, and speech segment duration patterns may be used for prosodic modeling. The prosodic features for at least a portion of an utterance are monitored over a period of time to determine a shape associated with the utterance. A score may be determined to assist in classifying the current utterance as human directed or computer directed without relying on knowledge of preceding utterances or utterances following the current utterance. Outside data may be used for training lexical addressee detection systems for the H-H-C scenario. H-C training data can be obtained from a single-user H-C collection and that H-H speech can be modeled using general conversational speech. H-C and H-H language models may also be adapted using interpolation with small amounts of matched H-H-C data.",20170807,Microsoft
148,"In some examples, a facility augments an audio-video sequence playback display with respect to a current playback position of the audio-video sequence within a time index range of the sequence. For a first portion of the time index range of the sequence containing the current playback position (“CPP”), the facility performs automatic voice transcription against the audio component to obtain speech text for at least one speaker. For a second portion of the time index range of the sequence containing the CPP, the facility performs automatic image recognition against the video component to obtain identifying information identifying at least one person, object, or location. Simultaneously with the sequence playback display and proximate to the sequence playback display, the facility displays one or more annotations each based upon (a) at least a portion of the obtained speech text, (b) at least a portion of the obtained identifying information, or (c) both.",20170721,Microsoft
149,"Systems, methods, and computer-executable instructions for approximating a softmax layer are disclosed. A small world graph that includes a plurality of nodes is constructed for a vocabulary of a natural language model. A context vector is transformed. The small world graph is searched using the transformed context vector to identify a top-K hypothesis. A distance from the context vector for each of the top-K hypothesis is determined. The distance is transformed to an original inner product space. A softmax distribution is computed for the softmax layer over the inner product space of the top-K hypothesis. The softmax layer is useful for determining a next word in a speech recognition or machine translation.",20180628,Microsoft
150,"Techniques (300, 400, 500) and apparatuses (100, 200, 700) for recognizing accented speech are described. In some embodiments, an accent module recognizes accented speech using an accent library based on device data, uses different speech recognition correction levels based on an application field into which recognized words are set to be provided, or updates an accent library based on corrections made to incorrectly recognized speech.",20190521,Google
151,"Non-limiting examples of the present disclosure relate to automated presentation control during presentation of one or more digital documents. Processing operations described herein may be configured to work with any type of application/service, where examples described herein extend to presentation of any type of digital document(s) including slide-based presentations. Speech signals may be detected while a digital document is being presented. The detected speech signals may be converted to text. The converted text may be matched with contextual data (e.g., pre-processed content) associated with the digital document. Probability scoring for determination of a next placement position of a digital document may be generated based on the matching of converted text with the contextual data of the digital document. The next placement position may be selected based on the generated probability scoring. Display of the digital document may be automatically updated to reflect the selected next placement position.",20180411,Microsoft
152,"Disclosed is an electronic device including a communication circuit, a microphone, a memory, speaker, and a processor, in which the processor receives the speech input and transmits first data associated with the speech input to a first server for supporting the speech recognition service, receives second data corresponding to processing of a part of the first data from the first server and outputs the second data at a first time that a first period of time has elapsed after the transmission of the first data, and outputs third data corresponding to processing of the rest of the first data at a second time that a second period of time has elapsed from the first time, while receiving the third data from the first server or a second server for supporting the speech recognition service before the second time.",20180718,SAMSUNG
153,"Text can be presented with speech indicators generated by a cognitive system by processing the text. The speech indicators can indicate recommended speech characteristics to be exhibited by a user while the user generates spoken utterances representing the text. Data indicating at least one user input changing at least one of the speech indicators from a first state as originally presented to a second state can be received. In response, a value indicating a level of change made to the at least one of the speech indicators can be determined. At least one parameter used by the cognitive system to select the speech indicators can be modified or created based on the value indicating the level of change made to the at least one of the speech indicators.",20171222,IBM
154,"Contemporary human-computer interactions include conversational interactions, wherein devices present conversational prompts (e.g., generated speech) and conversational responses to user inquiries (e.g., verbal user input). Presented herein are techniques for automatically assembling conversational representations of web content. A variety of automated assembly techniques are disclosed, such as conversational template for websites of various website types. Interactions of users with a website may be monitored to identify actions that the users frequently perform, and conversational interactions may be generated that correspond to the actions. A web service may present a set of requests, and conversational interactions may be assembled to match the respective requests and responses of the web service. Conversational interactions may include transitions between websites, and conversational representations may be merged to integrate content from multiple websites. Action sets of actions and associated conversational representations may be compiled to provide a conversational interaction that aggregates the capabilities of many websites.",20180131,Microsoft
155,"Systems are provided to facilitate continuous detection of words, names, phrases, or other sounds of interest and, responsive to such detection, provide a related user experience. The user experience can include providing links to media, web searches, translation services, journaling applications, or other resources based on detected ambient speech or other sounds. To preserve the privacy of those using and/or proximate to such systems, the system refrains from transmitting any information related to the detected sound unless the system receives permission from a user. Such permission can include the user interacting with a provided web search link, media link, or other user interface element.",20180409,Google
156,"An electronic device and method are disclosed herein. The electronic device includes a network interface and processor. The processor implements the method, including receiving a voice input through a network interface as transmitted from a first external device, including a request to execute a function using at least one application which is not indicated in the voice input, extracting a first text from the voice input by executing automatic speech recognition (ASR), when the at least one application is identified based on the first text, transmitting, through the network interface to the first external device, second data associated with the identified at least one application for display by the first external device, and when the at least one application is not identified based at least in part on the first text, reattempting identification of the at least one application by executing natural language understanding (NLU) on the first text.",20180709,SAMSUNG
157,"An approach is provided to detect pronouns that are included in textual posts that are found in an online discussion. The textual posts are analyzed using a natural language processing speech classification technique, that results in an identification of a noun to which the detected pronoun refers. The system then displays, on a display device, the noun to which the pronoun refers.",20171219,IBM
158,"A method, a system, and a computer program product are provided for interpreting low amplitude speech and transmitting amplified speech to a remote communication device. At least one computing device receives sensor data from multiple sensors. The sensor data is associated with the low amplitude speech. At least one of the at least one computing device analyzes the sensor data to map the sensor data to at least one syllable resulting in a string of one or more words. An electronic representation of the string of the one or more words may be generated and transmitted to a remote communication device for producing the amplified speech from the electronic representation.",20190815,IBM
159,"This document generally describes systems and methods for dynamically adapting speech recognition for individual voice queries of a user using class-based language models. The method may include receiving a voice query from a user that includes audio data corresponding to an utterance of the user, and context data associated with the user. One or more class models are then generated that collectively identify a first set of terms determined based on the context data, and a respective class to which the respective term is assigned for each respective term in the first set of terms. A language model that includes a residual unigram may then be accessed and processed for each respective class to insert a respective class symbol at each instance of the residual unigram that occurs within the language model. A transcription of the utterance of the user is then generated using the modified language model.",20190311,Google
160,"An improved translation experience is provided using an auxiliary device, such as a pair of earbuds, and a wirelessly coupled mobile device. Microphones on both the auxiliary device and the mobile device simultaneously capture input from, respectively, a primary user (e.g., wearing the auxiliary device) and a secondary user (e.g., a foreign language speaker providing speech that the primary user desires to translate). Both microphones continually listen, rather than alternating between the mobile device and the auxiliary device. Each device may determine when to endpoint and send a block of speech for translation, for example based on pauses in the speech. Each device may accordingly send the received speech for translation and output, such that it is provided in a natural flow of communication.",20190206,Google
161,"A computer-implemented method, a computer program product, and an incremental learning system are provided for language learning and speech enhancement. The method includes transforming acoustic utterances uttered by an individual into textual representations thereof, by a voice-to-language processor configured to perform speech recognition. The method further includes accelerating speech development in the individual, by an incremental learning system that includes the voice-to-language processor and that processes the acoustic utterances using natural language processing and analytics to determine and incrementally provide new material to the individual for learning.",20171025,IBM
162,"A method, a system, and a computer program product are provided for interpreting low amplitude speech and transmitting amplified speech to a remote communication device. At least one computing device receives sensor data from multiple sensors. The sensor data is associated with the low amplitude speech. At least one of the at least one computing device analyzes the sensor data to map the sensor data to at least one syllable resulting in a string of one or more words. An electronic representation of the string of the one or more words may be generated and transmitted to a remote communication device for producing the amplified speech from the electronic representation.",20171219,IBM
163,"Training a machine by a machine learning technique for recognizing speech utterance to determine language fluency level of a user. Native speaker recorded data and language specific dictionary of heteronyms may be retrieved. The native speaker recorded data may be parsed and the heteronyms from the native speaker recorded data may be isolated. Linguistic features from the native speaker recorded data including at least linguistic features associated with the heteronyms may be extracted, and a language dependent machine learning model is generated based on the linguistic features.",20171120,IBM
164,"The disclosed subject matter provides a system, computer readable storage medium, and a method providing an audio and textual transcript of a communication. A conferencing services may receive audio or audio visual signals from a plurality of different devices that receive voice communications from participants in a communication, such as a chat or teleconference. The audio signals representing voice (speech) communications input into respective different devices by the participants. A translation services server may receive over a separate communication channel the audio signals for translation into a second language. As managed by the translation services server, the audio signals may be converted into textual data. The textual data may be translated into text of different languages based the language preferences of the end user devices in the teleconference. The translated text may be further translated into audio signals.",20190709,Google
165,"A computer-implemented method and associated computing device for translating speech can include receiving, at a microphone of a computing device, an audio signal representing speech of a user in a first language or in a second language at a first time. A positional relationship between the user and the computing device at the first time can be determined and utilized to determine whether the speech is in the first language or the second language. The method can further include obtaining, at the computing device, a machine translation of the speech represented by the audio signal based on the determined language, wherein the machine translation is: (i) in the second language when the determined language is the first language, or (ii) in the first language when the determined language is the second language. An audio representation of the machine translation can be output from a speaker of the computing device.",20170925,Google
166,"An approach is provided to detect pronouns that are included in textual posts that are found in an online discussion. The textual posts are analyzed using a natural language processing speech classification technique, that results in an identification of a noun to which the detected pronoun refers. The system then displays, on a display device, the noun to which the pronoun refers.",20171106,IBM
167,"Aspects create a multimedia presentation wherein processors are configured to calculate a time it would take to narrate a plurality of words in a document at a specified speech speed in response to determining that the time it would take to narrate the plurality of words in the document at the specified speech speed exceeds a specified maximum time, generate a long summary of the document as a subset of the plurality of words, generate audio content for a first portion of the plurality of words of the long summary by applying a text-to-speech processing mechanism to the portion of the long summary at the desired speech speed, and create a multimedia slide of a multimedia presentation by adding the generated audio content to a presentation of text from a remainder portion of the plurality of words of the long summary.",20171102,IBM
168,"The subject matter of this specification can be embodied in, among other things, a method that includes receiving two or more data sets each representing speech of a corresponding individual attending an internet-based social networking video conference session, decoding the received data sets to produce corresponding text for each individual attending the internet-based social networking video conference, and detecting characteristics of the session from a coalesced transcript produced from the decoded text of the attending individuals for providing context to the internet-based social networking video conference session.",20181211,Google
169,"The present invention is a system and method for cognitive customer interaction. The system includes a humanoid robot having a processor operably connected to a camera and a speaker within the robot. The system has a social interactions database connected to the processor. The social interactions database stores interaction patterns representing speech, facial expressions, and body movement. An advanced analytics program, such as Watson Analytics, determines the identity of a customer in a store and performs social content analysis of the customer on social media platforms. The advanced analytics program generates a psycholinguistic profile of the user so gestures and speech of the customer is compared to the psycholinguistic profile and the humanoid robot may response appropriately.",20180223,IBM
170,"Predicting and learning users' intended actions on an electronic device based on free-form speech input. Users' actions can be monitored to develop a list of carrier phrases having one or more actions that correspond to the carrier phrases. A user can speak a command into a device to initiate an action. The spoken command can be parsed and compared to a list of carrier phrases. If the spoken command matches one of the known carrier phrases, the corresponding action(s) can be presented to the user for selection. If the spoken command does not match one of the known carrier phrases, search results (e.g., Internet search results) corresponding to the spoken command can be presented to the user. The actions of the user in response to the presented action(s) and/or the search results can be monitored to update the list of carrier phrases.",20190404,Google
171,"In an aspect of the present disclosure, a method for providing an alternate modality of input for filling a form field in response to a failure of voice recognition is disclosed including prompting the user for information corresponding to a field of a form, generating speech data by capturing a spoken response of the user to the prompt using at least one input device, attempting to convert the speech data to text, determining that the attempted conversion has failed, evaluating the failure using at least one speech rule, selecting, based on the evaluation, an alternate input modality to be used for receiving the information corresponding to the field of the form, receiving the information corresponding to the field of the form from the alternate input modality, and injecting the received information into the field of the form.",20190307,IBM
172,"A display apparatus and a method for questions and answers includes a display unit includes an input unit configured to receive user's speech voice; a communication unit configured to perform data communication with an answer server; and a processor configured to create and display one or more question sentences using the speech voice in response to the speech voice being a word speech, create a question language corresponding to the question sentence selected from among the displayed one or more question sentences, transmit the created question language to the answer server via the communication unit, and, in response to one or more answer results related to the question language being received from the answer server, display the received one or more answer results. Accordingly, the display apparatus may provide an answer result appropriate to a user's question intention although a non-sentence speech is input.",20190523,SAMSUNG
173,"Methods, systems, and computer programs are presented for providing a user interface (UI) for monitoring and debugging an Artificial Intelligence (AI) chatting hot. One method includes operations for receiving a selection on the UI to replay an electronic conversation between a first and a second party, selecting conversation data associated with the electronic conversation from a data log having conversation data from several electronic conversations, and analyzing the conversation data to identify conversation parameters. The conversation parameters include text in each entry of the electronic conversation, timing of the entries, and debugging parameters for each entry. The method further includes an operation for causing presentation of the electronic conversation on the UI, which includes presenting the text of each entry, the audio corresponding to speech associated with each entry timed according to the timing of the entry, and the debugging parameters embedded within the presented text.",20170918,Microsoft
174,"In some implementations, a language proficiency of a user of a client device is determined by one or more computers. The one or more computers then determines a text segment for output by a text-to-speech module based on the determined language proficiency of the user. After determining the text segment for output, the one or more computers generates audio data including a synthesized utterance of the text segment. The audio data including the synthesized utterance of the text segment is then provided to the client device for output.",20180919,Google
175,"The techniques describe a tool that finds people to help answer a question that arises while a user is consuming content of a file. As the user consumes the content, the techniques identify a signal that indicates an issue (e.g., a question). The signal can be added or somehow inserted into the underlying content of the file based on user input. The techniques determine a portion of the content associated with the signal. The portion of the content, along with any other information provided via the user input, is analyzed to determine a context of the issue. For example, natural language processing techniques can parse text or speech to understand the subject matter. The techniques then access a resolution resource (e.g., a professional network) to identify people that are likely capable of providing assistance, and the people are suggested to the user.",20180209,Microsoft
176,"A method and workstation for generating a transcript of a conversation between a patient and a healthcare practitioner is disclosed. A workstation is provided with a tool for rendering of an audio recording of the conversation and generating a display of a transcript of the audio recording using a speech-to-text engine, thereby enabling inspection of the accuracy of conversion of speech to text. A tool is provided for scrolling through the transcript and rendering the portion of the audio according to the position of the scrolling. There is a highlighting in the transcript of words or phrases spoken by the patient relating to symptoms, medications or other medically relevant concepts. Additionally, there is provided a set of transcript supplement tools enabling editing of specific portions of the transcript based on the content of the corresponding portion of audio recording.",20180524,Google
177,Correcting typographical errors in electronic text may include converting a text message containing at least one phonemic spelling of a word into speech by running a text-to-speech application programming interface (API) with the text message as input. The converted speech may be input to a speech-to-text API and the speech-to-text API executed to convert the speech to text. A text file comprising the text may be generated and/or output. The text file automatically contains a corrected version of the phonemic spelling of the word in text message.,20180320,IBM
178,"Aspects create a multimedia presentation wherein processors are configured to calculate a time it would take to narrate a plurality of words in a document at a specified speech speed in response to determining that the time it would take to narrate the plurality of words in the document at the specified speech speed exceeds a specified maximum time, generate a long summary of the document as a subset of the plurality of words, generate audio content for a first portion of the plurality of words of the long summary by applying a text-to-speech processing mechanism to the portion of the long summary at the desired speech speed, and create a multimedia slide of a multimedia presentation by adding the generated audio content to a presentation of text from a remainder portion of the plurality of words of the long summary.",20190812,IBM
179,"According to one embodiment, a method, computer system, and computer program product for ancillary speech generation is provided. The present invention may include receiving a query from a querent; interpreting the terms of the query using schema information; rewriting the initial query into a set of related queries; pruning related queries that are irrelevant based on contemporary circumstances; retrieving preferences associated with the querent, where each of the preferences is assigned a weight representing the importance of that preference to the querent; filtering out the pruned related queries that do not comport with the querent's preferences; sort the filtered related queries according to the weight of the preferences served by each query; answering the filtered related queries with a knowledge graph; returning answers to the querent; and updating the preferences based on the answers.",20170830,IBM
180,"The subject matter of this specification can be embodied in, among other things, a method that includes receiving two or more data sets each representing speech of a corresponding individual attending an internet-based social networking video conference session, decoding the received data sets to produce corresponding text for each individual attending the internet-based social networking video conference, and detecting characteristics of the session from a coalesced transcript produced from the decoded text of the attending individuals for providing context to the internet-based social networking video conference session.",20191030,Google
181,"A method for eyes-off training of a dictation system includes translating an audio signal featuring speech audio of a speaker into an initial recognized text using a previously-trained general language model. The initial recognized text is provided to the speaker for error correction. The audio signal is re-translated into an updated recognized text using a specialized language model biased to recognize words included in the corrected text. The general language model is retrained in an “eyes-off” manner, based on the audio signal and the updated recognized text.",20180716,Microsoft
182,"A computing system and related techniques for selecting content to be automatically converted to speech and provided as an audio signal are provided. A text-to-speech request associated with a first document can be received that includes data associated with a playback position of a selector associated with a text-to-speech interface overlaid on the first document. First content associated with the first document can be determined based at least in part on the playback position, the first content including content that is displayed in the user interface at the playback position. The first document can be analyzed to identify one or more structural features associated with the first content. Speech data can be generated based on the first content and the one or more structural features.",20190521,Google
183,"Systems and methods for programmatic representation of natural language patterns are disclosed. A method includes accessing, via an electronic transmission, a text in a natural language. The method includes identifying, based on a plurality of stored natural language patterns residing in a data repository, one or more word groups within the text, each word group corresponding to at least one stored natural language pattern, each stored natural language pattern corresponding to a grammatical part of speech or a word-phrase type in the natural language. The method includes providing an output representing the identified one or more word groups and the at least one stored natural language pattern corresponding to each of the identified one or more word groups.",20180912,Microsoft
184,"Methods, systems, and computer programs are presented for a smart communications assistant with an audio interface. One method includes an operation for getting messages addressed to a user. The messages are from one or more message sources and each message comprising message data that includes text. The method further includes operations for analyzing the message data to determine a meaning of each message, for generating a score for each message based on the respective message data and the meaning of the message, and for generating a textual summary for the messages based on the message scores and the meaning of the messages. A speech summary is created based on the textual summary and the speech summary is then sent to a speaker associated with the user. The audio interface further allows the user to verbally request actions for the messages.",20191115,Microsoft
185,"An electronic device for performing speech recognition and a method therefor are provided. The method includes detecting a first text, which is preset for performing speaker recognition, by performing speech recognition on a first speech signal, performing speaker recognition on a second speech signal acquired after the first speech signal, based on the first text being detected, and executing a voice command obtained from the second speech signal, based on a result of performing the speaker recognition on the second speech signal indicating that a speaker of the second speech signal corresponds to a first speaker who registered the first text.",20190820,SAMSUNG
186,"A method of updating a grammar model used during speech recognition includes obtaining a corpus including at least one word, obtaining the at least one word from the corpus, splitting the at least one obtained word into at least one segment, generating a hint for recombining the at least one segment into the at least one word, and updating the grammar model by using at least one segment comprising the hint.",20191025,SAMSUNG
187,"An electronic device for providing one or more items to a user in response to a user speech and a system therefor are provided. The electronic device and the system search one or more items in response to a user's voice command related with a search of an item. In case where items are searched, a user performs various activities related with the items. The items each include objects. The electronic device and the system identify an object that is used when the user identifies at least one item (for example, an item preferred by the user) among the items from the activity. By matching the identified object and a feature of the identified object, the electronic device and the system determine a user's preference related with a search of an item. The identified preference is used for sorting of the items.",20190807,SAMSUNG
188,"Among other things, embodiments of the present disclosure may be used to help train speech recognizers for improving generalized voice experience quality in a chat bot system. In some embodiments, the system provides users with games to play to increase user engagement with the chat bot system.",20190925,Microsoft
189,"Methods, computer program products, and systems are presented. The methods include, for instance: obtaining a media file including a speech by one or more speaker. The language of the speech is identified and biographic data of a speaker of the speech is generated by analyzing semantics and vocal characteristics of the speech. The speaker is diarized and confidence in a resulting speaker label is evaluated against a threshold. The speaker label is adjusted with the language of the speech and biographic data of the speaker and produced as speaker metadata of the media file.",20190913,IBM
190,"Mechanisms are provided to implement an efficient translating mechanism to efficiently translating social media posts. A source language to be used to translate the social media post is identified based on words within the social media post. A highest classification is identified and the social media post is translated from the source language to a target language using a translation level associated with the highest classification. In the translation, each word and its related meaning in the target language are identified from a multi-language data structure; each word is categorized into its associated part of speech; a sentence is generated in the target language; and natural language processing is performed on each sentence in the target language to identity the existence of ambiguous connotations. Responsive to each sentence failing have any ambiguous connotations, a social medial post is generated in the target language utilizing the generated sentences.",20180914,IBM
191,"In some implementations, a language proficiency of a user of a client device is determined by one or more computers. The one or more computers then determines a text segment for output by a text-to-speech module based on the determined language proficiency of the user. After determining the text segment for output, the one or more computers generates audio data including a synthesized utterance of the text segment. The audio data including the synthesized utterance of the text segment is then provided to the client device for output.",20190917,Google
192,"A method for natural language interaction includes recording speech provided by a human user. The recorded speech is translated into a machine-readable natural language input relating to an interaction topic. An interaction timer is maintained that tracks a length of time since a last machine-readable natural language input referring to the interaction topic was translated. Based on a current value of the interaction timer being greater than an interaction engagement threshold, a message relating to the interaction topic is delivered with a first natural language phrasing that includes an interaction topic reminder. Based on the current value of the interaction timer being less than the interaction engagement threshold, the message relating to the interaction topic is delivered with a second natural language phrasing that lacks the interaction topic reminder.",20191011,Microsoft
193,An automatic dubbing method is disclosed. The method comprises: extracting speeches of a voice from an audio portion of a media content (504); obtaining a voice print model for the extracted speeches of the voice (506); processing the extracted speeches by utilizing the voice print model to generate replacement speeches (508); and replacing the extracted speeches of the voice with the generated replacement speeches in the audio portion of the media content (510).,20161121,Microsoft
194,"Techniques are presented for providing misinformation detection in online content. The described techniques can identify instances of misinformation in online content and pass a misinformation result to the user. A misinformation probability analysis can be performed by applying a syntactic analysis and a semantic analysis to detect misinformation with confidence by applying featurization to a URL, text of content referenced by the URL, and metadata associated with the URL using a feature set, the feature set comprising semantic-based features and syntactic-based features, wherein the semantic features and the syntactic features are selected from the group consisting of: sentiment amplifiers, sentiment continuity disruption features, lexical features, keywords, baseline features, speech act, sensicon features, emotion detection on the obtained text, exaggerated language, strong adjectives, heuristics, bag-of-words, objectivity, colloquial-ness score, and semantic difference.",20180627,Microsoft
195,"A computer-implemented method for training a neural network that is configured to generate a score distribution over a set of multiple output positions. The neural network is configured to process a network input to generate a respective score distribution for each of a plurality of output positions including a respective score for each token in a predetermined set of tokens that includes n-grams of multiple different sizes. Example methods described herein provide trained neural networks which produce results with improved accuracy compared to the state of the art, e.g. translations that are more accurate compared to the state of the art, or more accurate speech recognition compared to the state of the art.",20171003,Google
196,"Embodiments provide a system and method for filtering speech in a video. Speech in video may contain objectionable words that need to be filtered. To ascertain whether a word or phrase is objectionable, the contextual information from surrounding words and the contextual information from detected objects and scenes in the video are used. Unwanted words may be filtered or collected and presented to the user.",20180917,IBM
197,"Techniques described herein relate to facilitating end-to-end multilingual communications with automated assistants. In various implementations, speech recognition output may be generated based on voice input in a first language. A first language intent may be identified based on the speech recognition output and fulfilled in order to generate a first natural language output candidate in the first language. At least part of the speech recognition output may be translated to a second language to generate an at least partial translation, which may then be used to identify a second language intent that is fulfilled to generate a second natural language output candidate in the second language. Scores may be determined for the first and second natural language output candidates, and based on the scores, a natural language output may be selected for presentation.",20200217,Google
198,"The subject matter of this specification can be embodied in, among other things, a method that includes receiving two or more data sets each representing speech of a corresponding individual attending an internet-based social networking video conference session, decoding the received data sets to produce corresponding text for each individual attending the internet-based social networking video conference, and detecting characteristics of the session from a coalesced transcript produced from the decoded text of the attending individuals for providing context to the internet-based social networking video conference session.",20200518,Google
199,"A user activity pattern may be ascertained using signal data from a set of computing devices. The activity pattern may be used to infer user intent with regards to a user interaction with a computing device or to predict a likely future action by the user. In one implementation, a set of computing devices is monitored to detect user activities using sensors associated with the computing devices. Activity features associated with the detected user activities are determined and used to identify an activity pattern based on a plurality of user activities having similar features. Examples of user activity patterns may include patterns based on time, location, content, or other context. The inferred user intent or predicted future actions may be used to facilitate understanding user speech or determining a semantic understanding of the user.",20200903,Microsoft
200,"Embodiments provide a system and method for filtering speech in a video. Speech in video may contain objectionable or profane words that need to be filtered. To ascertain whether a word or phrase is objectionable, the contextual information from surrounding words and the contextual information from detected objects and scenes in the video are used. Unwanted words may be filtered or collected and presented to the user.",20200908,IBM
201,"A method of transcribing speech using a multilingual end-to-end (E2E) speech recognition model includes receiving audio data for an utterance spoken in a particular native language, obtaining a language vector identifying the particular language, and processing, using the multilingual E2E speech recognition model, the language vector and acoustic features derived from the audio data to generate a transcription for the utterance. The multilingual E2E speech recognition model includes a plurality of language-specific adaptor modules that include one or more adaptor modules specific to the particular native language and one or more other adaptor modules specific to at least one other native language different than the particular native language. The method also includes providing the transcription for output.",20200330,Google
202,A method includes receiving a speech input from a user and obtaining context metadata associated with the speech input. Hie method also includes generating a raw speech recognition result corresponding to the speech input and selecting a list of one or more denormalizers to apply to the generated raw speech recognition result based on the context metadata associated with the speech input. The generated raw speech recognition result includes normalized text. The method also includes denormalizing the generated raw speech recognition result into denormalized text by applying the list of the one or more denormalizers in sequence to the generated raw speech recognition result.,20200901,Google
203,"An electronic device and a method for controlling the electronic device thereof are provided. The electronic device includes a memory storing at least one instruction, and a processor configured to control the electronic device by executing the at least one instruction stored in the memory, and the processor is configured to, based on a user's speech being input, acquire a first sentence in a first language corresponding to the user's speech through a speech recognition model corresponding to a language of the user's speech, acquire a second sentence in a second language corresponding to the first sentence in the first language through a machine translation model trained to translate a plurality of languages into the predefined second language, and acquire a control instruction of the electronic device corresponding to the acquired second sentence or acquire a response to the second sentence through a natural language understanding model trained based on the second language. In particular, the electronic device uses an artificial intelligence model trained according to at least one of machine learning, neural network or deep learning algorithms in at least some part of the method for acquiring a control instruction or a response regarding the user's speech.",20200402,SAMSUNG
204,"A method for aligning a translation of original caption data with an audio portion of a video is provided. The method includes identifying, by a processing device, original caption data for a video that includes a plurality of caption character strings. The processing device identifies speech recognition data that includes a plurality of generated character strings and associated timing information for each generated character string. The processing device maps the plurality of caption character strings to the plurality of generated character strings using assigned values indicative of semantic similarities between character strings. The processing device assigns timing information to the individual caption character strings based on timing information of mapped individual generated character strings. The processing device aligns a translation of the original caption data with the audio portion of the video using assigned timing information of the individual caption character strings.",20180226,Google
205,"The subject matter of this specification can be implemented in, among other things, a computer-implemented method for correcting words in transcribed text including receiving speech audio data from a microphone. The method further includes sending the speech audio data to a transcription system. The method further includes receiving a word lattice transcribed from the speech audio data by the transcription system. The method further includes presenting one or more transcribed words from the word lattice. The method further includes receiving a user selection of at least one of the presented transcribed words. The method further includes presenting one or more alternate words from the word lattice for the selected transcribed word. The method further includes receiving a user selection of at least one of the alternate words. The method further includes replacing the selected transcribed word in the presented transcribed words with the selected alternate word.",20200421,Google
206,"This document generally describes systems and methods for dynamically adapting speech recognition for individual voice queries of a user using class-based language models. The method may include receiving a voice query from a user that includes audio data corresponding to an utterance of the user, and context data associated with the user. One or more class models are then generated that collectively identify a first set of terms determined based on the context data, and a respective class to which the respective term is assigned for each respective term in the first set of terms. A language model that includes a residual unigram may then be accessed and processed for each respective class to insert a respective class symbol at each instance of the residual unigram that occurs within the language model. A transcription of the utterance of the user is then generated using the modified language model.",20191231,Google
207,"A system can be configured to perform tasks such as converting recorded speech to a sequence of phonemes that represent the speech, converting an input sequence of graphemes into a target sequence of phonemes, translating an input sequence of words in one language into a corresponding sequence of words in another language, or predicting a target sequence of words that follow an input sequence of words in a language (e.g., a language model). In a speech recognizer, the RNN system may be used to convert speech to a target sequence of phonemes in real-time so that a transcription of the speech can be generated and presented to a user, even before the user has completed uttering the entire speech input.",20200204,Google
208,"Methods, systems, and apparatus, including computer programs encoded on computer storage media for speech recognition. One method includes obtaining an input acoustic sequence, the input acoustic sequence representing an utterance, and the input acoustic sequence comprising a respective acoustic feature representation at each of a first number of time steps, processing the input acoustic sequence using a first neural network to convert the input acoustic sequence into an alternative representation for the input acoustic sequence, processing the alternative representation for the input acoustic sequence using an attention-based Recurrent Neural Network (RNN) to generate, for each position in an output sequence order, a set of substring scores that includes a respective substring score for each substring in a set of substrings; and generating a sequence of substrings that represent a transcription of the utterance.",20191213,Google
209,"A method and workstation for generating a transcript of a conversation between a patient and a healthcare practitioner is disclosed. A workstation is provided with a tool for rendering of an audio recording of the conversation and generating a display of a transcript of the audio recording using a speech-to-text engine, thereby enabling inspection of the accuracy of conversion of speech to text. A tool is provided for scrolling through the transcript and rendering the portion of the audio according to the position of the scrolling. There is a highlighting in the transcript of words or phrases spoken by the patient relating to symptoms, medications or other medically relevant concepts. Additionally, there is provided a set of transcript supplement tools enabling editing of specific portions of the transcript based on the content of the corresponding portion of audio recording.",20200623,Google
